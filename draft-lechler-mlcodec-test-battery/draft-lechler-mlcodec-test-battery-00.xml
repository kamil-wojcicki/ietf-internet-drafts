<?xml version='1.0' encoding='utf-8'?>
<!DOCTYPE rfc [
  <!ENTITY nbsp    "&#160;">
  <!ENTITY zwsp   "&#8203;">
  <!ENTITY nbhy   "&#8209;">
  <!ENTITY wj     "&#8288;">
]>
<?xml-stylesheet type="text/xsl" href="rfc2629.xslt" ?>
<!-- generated by https://github.com/cabo/kramdown-rfc version 1.7.29 (Ruby 3.2.3) -->
<rfc xmlns:xi="http://www.w3.org/2001/XInclude" ipr="trust200902" docName="draft-lechler-mlcodec-test-battery-00" category="info" consensus="true" submissionType="IETF" tocInclude="true" sortRefs="true" symRefs="true" version="3">
  <!-- xml2rfc v2v3 conversion 3.30.0 -->
  <front>
    <title abbrev="MlCodecTestBattery">Test Battery for Opus ML Codec Extensions</title>
    <seriesInfo name="Internet-Draft" value="draft-lechler-mlcodec-test-battery-00"/>
    <author fullname="Laura Lechler">
      <organization>Cisco Systems</organization>
      <address>
        <postal>
          <country>United Kingdom</country>
        </postal>
        <email>llechler@cisco.com</email>
      </address>
    </author>
    <author fullname="Kamil Wojcicki">
      <organization>Cisco Systems</organization>
      <address>
        <postal>
          <country>Australia</country>
        </postal>
        <email>kamilwoj@cisco.com</email>
      </address>
    </author>
    <date year="2025" month="July" day="20"/>
    <area>Applications and Real-Time</area>
    <workgroup>Machine Learning for Audio Coding</workgroup>
    <keyword>mushra</keyword>
    <keyword>drt</keyword>
    <keyword>evaluation</keyword>
    <abstract>
      <?line 200?>

<t>This document proposes methodology and data for evaluation of machine learning (ML) codec extensions,
such as the deep audio redundancy (DRED), within the Opus codec (RFC6716).</t>
    </abstract>
    <note removeInRFC="true">
      <name>About This Document</name>
      <t>
        Status information for this document may be found at <eref target="https://datatracker.ietf.org/doc/draft-lechler-mlcodec-test-battery/"/>.
      </t>
      <t>
        Discussion of this document takes place on the
        Machine Learning for Audio Coding Working Group mailing list (<eref target="mailto:mlcodec@ietf.org"/>),
        which is archived at <eref target="https://mailarchive.ietf.org/arch/browse/mlcodec/"/>.
        Subscribe at <eref target="https://www.ietf.org/mailman/listinfo/mlcodec/"/>.
      </t>
    </note>
  </front>
  <middle>
    <?line 206?>

<section anchor="introduction">
      <name>Introduction</name>
      <t>The IETF machine learning for audio coding (mlcodec) working group aims to 
leverage current and future opportunities presented by ML codecs 
to enhance the Opus codec <xref target="RFC6716"/> and its extensions, 
including to improve speech coding quality and robustness to packet loss. 
Effective evaluation of codec extensions (such as DRED),
in both standalone and redundancy settings,
is a crucial factor in achieving those objectives.
It supports reproducibility for existing extensions 
(for instance, by enabling validation of whether a retraining pipeline matches baseline model performance)
and enables benchmarking of future improvements against previously established baselines.</t>
      <t>However, as outlined in subsequent sections, 
effective evaluation of generative ML models presents 
numerous challenges and necessitates specialized subjective 
and objective evaluation methods. 
This document proposes a crowdsourced subjective test battery,
along with associated test datasets, to address the unique requirements 
for accurate and reproducible evaluations of ML codecs.
The proposed test battery covers both speech quality and intelligibility, 
including tests in clean, noisy, and reverberant conditions, 
and incorporates real-world audio data. 
The methodology leverages crowdsourced listeners <xref target="CROWDSOURCED-DRT"/> 
to enable rapid and scalable assessments, 
while controlling the variability associated with non-lab-based measurements.</t>
      <t>In the era of generative ML models, 
reference-based objective metrics face additional limitations, 
while non-intrusive methods struggle with generalization, e.g., <xref target="URGENT2025"/> and <xref target="CROWDSOURCED-MUSHRA"/>. 
Consequently, the use of human listeners, 
the gold standard in both quality and intelligibility assessment, 
is of notable importance.
The generative nature of ML codecs also implies that speech intelligibility 
could be significantly improved and/or degraded by such algorithms. 
For example, human perception for some phoneme categories could be enhanced, 
while confusions might be introduced for others, 
including hallucinations of incorrect phonemes even at high overall perceived quality.
Such confusions may not be easily detected in quality tests, 
highlighting a pressing need for highly diagnostic phoneme-category, 
or even phoneme-level, intelligibility assessment methods.</t>
      <t>The subsequent sections present the methodology, key considerations, 
and further motivation underlying the proposed test battery, 
addressing the challenges and requirements discussed above.</t>
      <section anchor="listening-test-methods">
        <name>Listening Test Methods</name>
        <section anchor="mushra-1s">
          <name>MUSHRA 1S</name>
          <t>MUSHRA 1S is variant of the well-established MUSHRA (multiple stimuli with hidden reference and anchor) methodology for assessing quality <xref target="ITU-R.BS1534-3"/> in clean non-reverberant conditions is proposed for testing and benchmarking of ML codecs. MUSHRA is firstly adapted to a crowdsourced, non-expert listener base, as described in <xref target="CROWDSOURCED-MUSHRA"/>. Particularly for generative models, which may cause hallucinations, a reference-based listening test is preferable <xref target="URGENT2025"/>. Secondly, one system under test is assessed at a time, in the context of a fixed reference and anchor. The advantages of testing one system at a time are the unlimited extendability of test conditions within the quality range of anchor and reference, avoiding context effects of other conditions within the same test, avoiding difficulties associated when merging results across multiple tests, and simplifying the task for the participants thereby avoiding listener fatigue, particularly in non-expert listeners. As such, MUSHRA 1S has similarities with to the absolute category ratings (ACR) tests, which can be used to calculate a mean opinion scores (MOS), in that it is simple and easily extendable, while also being more stable than ACR, due to the fixed range of expected audio quality, bound by the anchor and reference. Reference-less MOS scores have been demonstrated to suffer from range-equalizing biases <xref target="COOPER2023"/>, with other samples presented within the same test defining the range of expectation of what constitutes "good" or "bad" speech quality. The drawback of the MUSHRA 1S solution, compared to a traditional MUSHRA test, is the slightly decreased sensitivity to very small differences between similar methods, which may only be detectable in direct comparisons.</t>
        </section>
        <section anchor="dcr">
          <name>DCR</name>
          <t>The degradation category rating (DCR) approach is used to produce a degradation mean opinion score (DMOS) <xref target="ITU-T.P800"/>. Although it is typically used with a high-quality reference, the test is also capable of assessing degradation caused by codecs when tested on mild-to-moderately impaired real-world data <xref target="MULLER2024"/>. The approach is more sensitive than absolute category ratings (ACR) <xref target="ITU-T.P800"/>. An implementation of the test procedure for crowdsourced tests is available in <xref target="ITU-T.P808"/>.</t>
        </section>
        <section anchor="drt">
          <name>DRT</name>
          <t>The diagnostic rhyme test (DRT) <xref target="ITU-T.P807"/> measures speech intelligibility by presenting minimal pairs where the contrasted phonemes differ in terms of a specific, controlled phonetic category. The linguistic and acoustic insight of the DRT, with test items belonging to classes of distinctive
linguistic features which are acoustically interpretable, poses a useful tool for both codec analysis and benchmarking. The test is free from context-effects and memory effects and has a high test sensitivity. It is therefore well-suited for a crowdsourced listener audience. Bearing in mind the principles for crowdsourcing listening tests employed in <xref target="ITU-T.P808"/>, the test was adapted for crowdsourced listening tests in <xref target="CROWDSOURCED-DRT"/> and test vectors in five languages were published <xref target="DRT-REPO"/>. The test data was recently adopted by <xref target="LESCHANOWSKY2025"/>.</t>
        </section>
        <section anchor="crowdsourcing-adaptations">
          <name>Crowdsourcing Adaptations</name>
          <t>Crowdsourced listening tests benefit from rigorous screening and quality control. In addition to the specific implementation of standardized test approaches for crowdsourced listening tests, <xref target="ITU-T.P808"/> has provided useful guiding principles for the adaptation of laboratory-based tests to counteract challenges  posed by the comparatively uncontrolled crowdsourcing environment. For instance, steps of qualification and training are added before the actual test stimuli are presented and catch trials are included in the pool of test questions.
It is further recommended to assess the quality of participants' responses across different platforms, such as Amazon Mechanical Turk, Prolific, and others <xref target="CROWDSOURCED-MUSHRA"/>. Each platform has a unique set of filters that can be used to recruit a specific participant pool. The platform and any filters used should always be reported along with test results, as absolute results may depend on those settings and may differ considerably between platforms.</t>
        </section>
      </section>
    </section>
    <section anchor="proposed-crowdsourced-listening-test-battery">
      <name>Proposed Crowdsourced Listening Test Battery</name>
      <t>In the literature, evaluations of speech codec quality often focus solely on clean conditions. 
However, given the wide range of potential applications for modern speech codecs, 
and the unique ways in which ML codecs may be affected by various types of real-world distortions,
it is important to assess their limitations under representative real-world scenarios, 
including challenging listening conditions.</t>
      <t>In addition to clean speech data, the proposed test battery considers performance evaluation on overlapping speech, reverberant and noisy speech, speaker consistency and phoneme-level intelligibility. The current version comprises predominantly English test vectors, but the extension to include multiple languages is desirable.
Some of the modules of the test battery outlined below for assessment of standalone ML codec performance can also be used (where applicable), for assessing the performance of redundancy schemes under packet loss conditions (e.g., Opus+DRED).</t>
      <t>All proposed test vectors will be made publicly available at the sampling rate of 48 kHz.</t>
      <section anchor="speech-quality-evaluation">
        <name>Speech Quality Evaluation</name>
        <section anchor="clean-speech-test-vectors">
          <name>Clean Speech Test Vectors</name>
          <t>By employing the MUSHRA 1S approach and utilizing high-quality clean speech data, the system under test is evaluated with respect to the overall quality. The reference allows the listener to assess also the correctness of the linguistic content as well as the preservation of the speaker characteristics. In this test, the quality of each codec or extension is assessed in standalone mode. The diverse test set comprises 100 gender-balanced clean speech files, covering 100 unique speakers, and includes samples from both adult and children's speech. Furthermore, the set of test vectors covers a diverse range of accents of English.</t>
        </section>
        <section anchor="real-world-degradation-test-vectors">
          <name>Real-World Degradation Test Vectors</name>
          <t>As speech codecs may be used by a wide variety of applications, it cannot be ensured that the audio to be compressed constitutes clean speech in the sense of dry and noise-free high-quality audio. It is therefore important to assess the codec's resilience to real-world degradation. 
For tests where test vectors have impaired quality, DCR offers an effective way to measure the severity of any additional degradation introduced by the codec. 
The test data consists of 90 crowdsourced speech files in mildly impaired real-world scenarios of noise and reverberation. Of these, 45 files are predominantly focussed on reverberant speech and 45 on speech in noise. The reverberation and noise levels are mild to moderate.</t>
        </section>
        <section anchor="simultaneous-talker-test-vectors">
          <name>Simultaneous Talker Test Vectors</name>
          <t>Most application purposes rely on the codec's capability of preserving simultaneously occurring speech from multiple talkers. However, in practice, this can be a challenging task. A listening test using the DCR methodology offers insights into whether the presence of overlapping speech leads to degradation, which may occur in the form of artifacts or speech suppression. The proposed test set consists of 20 files of conversations between two to three talkers.</t>
        </section>
        <section anchor="packet-loss-scenarios">
          <name>Packet Loss Scenarios</name>
          <t>Real-world packet loss traces and/or simulated loss patterns (including using the packet loss simulator provided by the working group in Opus) can be utilized to evaluate the overall quality of redundancy codecs, such as Opus and DRED working together.</t>
          <t>Details TBD.</t>
        </section>
      </section>
      <section anchor="speech-intelligibility-evaluation">
        <name>Speech Intelligibility Evaluation</name>
        <section anchor="clean-speech-test-vectors-1">
          <name>Clean Speech Test Vectors</name>
          <t>The DRT for evaluating speech intelligibility, adapted for crowdsourced participants <xref target="CROWDSOURCED-DRT"/>, is proposed to be performed on a subset of the stimuli provided in <xref target="DRT-REPO"/>. The subset consists of two test vectors, one male and one female talker sample, for each word pair in the standard DRT word list for English <xref target="ITU-T.P807"/>. Test vectors for four other languages are also available in the same collection. 
Due to listeners' perceptual sensitivity to the subtle and highly localized cues that distinguish the two target phonemes, this test is primarily applicable in the evaluation of standalone codecs, with limited expected utility when combined with packet losses and redundancy schemes.</t>
        </section>
        <section anchor="noisy-test-vectors">
          <name>Noisy Test Vectors</name>
          <t>In order to evaluate a codec's resilience to noise in terms of speech intelligibility, the proposed evaluation battery for ML codecs contains noisy counterparts for the clean speech test vectors described in the previous paragraph. Speech-shaped noise (SSN) is used as a stationary additive masker in which intelligibility can be evaluated. While the presence of noise may lead to particularly severe codec distortion in some models, even the presence of well-preserved noise can help to distinguish the intelligibility of high-quality models that demonstrate a ceiling effect in clean conditions. The use of stationary noise is essential for the DRT to ensure uniform effects on the short-term localized perceptual cues. For the same reason, the noisy version of the test is also geared towards the evaluation of standalone codecs. 
The SSN was generated based on long-term-averaged short-term spectra of a publicly available clean speech data set <xref target="DEMIRSAHIN2020"/>. 
The average spectrum was used as a filter that was convolved with white noise, resulting in SSN.</t>
        </section>
      </section>
      <section anchor="example-results">
        <name>Example Results</name>
        <t>The results shown in Table 1 below were obtained by using test methodology described above. Subjective tests were run on the Prolific crowdsourcing platform. The participants were required to be native speakers of English, with an approval rate of at least 98% and at least 110 previous submissions. Only participants without any self-reported hearing impairments and without a cochlear implant were invited to participate. Additionally, diagnostic rhyme test studies were only open to participants who self-reported not to have have dyslexia.</t>
        <table>
          <thead>
            <tr>
              <th align="left">Codec</th>
              <th align="center">Quality in Clean Speech (MUSHRA) [95% CI]</th>
              <th align="center">Intelligibility in Clean Speech (DRT Score) [95% CI]</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left">Clean input</td>
              <td align="center">98.3 [+/- 0.2]</td>
              <td align="center">94.9 [+/- 1.3]</td>
            </tr>
            <tr>
              <td align="left">Opus v1.5.2 9kbps NOLACE</td>
              <td align="center">85.4 [+/- 1.7]</td>
              <td align="center">90.0 [+/- 2.0]</td>
            </tr>
            <tr>
              <td align="left">Opus v1.5.2 9kbps LACE</td>
              <td align="center">70.2 [+/- 2.0]</td>
              <td align="center">90.6 [+/- 1.8]</td>
            </tr>
            <tr>
              <td align="left">Opus v1.5.2 9kbps</td>
              <td align="center">56.2 [+/- 2.3]</td>
              <td align="center">89.0 [+/- 2.0]</td>
            </tr>
            <tr>
              <td align="left">Opus v1.5.2 6kbps</td>
              <td align="center">24.0 [+/- 0.7]</td>
              <td align="center">86.3 [+/- 2.4]</td>
            </tr>
            <tr>
              <td align="left">DRED SA 2kbps</td>
              <td align="center">64.9 [+/- 2.3]</td>
              <td align="center">88.4 [+/- 2.4]</td>
            </tr>
            <tr>
              <td align="left">DRED SA 1kbps</td>
              <td align="center">52.0 [+/- 2.4]</td>
              <td align="center">84.5 [+/- 2.8]</td>
            </tr>
            <tr>
              <td align="left">DRED SA 0.5kbps</td>
              <td align="center">20.7 [+/- 2.2]</td>
              <td align="center">71.7 [+/- 3.8]</td>
            </tr>
          </tbody>
        </table>
      </section>
    </section>
    <section anchor="objective-evaluation">
      <name>Objective Evaluation</name>
      <t>Objective metrics are often used during the development of speech codecs, 
with expert evaluations conducted towards the end of the development lifecycle. 
While effective for traditional DSP-based codecs, 
traditional well-established reference-based metrics, 
such as PESQ <xref target="ITU-T.P862"/>, often fail to accurately evaluate generative methods.
For instance, PESQ has been empirically shown to have an underestimation bias 
for generative models which may have high output quality but for which 
the output may also considerably differ from the reference <xref target="CROWDSOURCED-MUSHRA"/>.</t>
      <t>At present, the research into alternative metrics is flourishing 
with various innovative methods being proposed,<br/>
such as non-intrusive DNN-based metrics (e.g, <xref target="UTMOS"/>), 
metrics with non-matched references (e.g., <xref target="SCOREQ"/>), 
or composite score types of metrics (e.g., <xref target="UNI-VERSA"/>). 
While recent correlation investigations, e.g., <xref target="URGENT2025"/>, are promising, 
it is too early to include such metrics in this proposal, 
as it is yet to be seen which metrics can demonstrate both good accuracy and generalization 
to a variety of generative models and test vectors. 
Further insights in this area are of potential value for rapid, 
accessible and inexpensive evaluation of ML codecs. 
Hence, we propose to investigate which objective metrics are effective 
predictors of listener responses for the test battery components, 
and under which conditions.</t>
    </section>
    <section anchor="conventions-and-definitions">
      <name>Conventions and Definitions</name>
      <t>The key words "<bcp14>MUST</bcp14>", "<bcp14>MUST NOT</bcp14>", "<bcp14>REQUIRED</bcp14>", "<bcp14>SHALL</bcp14>", "<bcp14>SHALL
NOT</bcp14>", "<bcp14>SHOULD</bcp14>", "<bcp14>SHOULD NOT</bcp14>", "<bcp14>RECOMMENDED</bcp14>", "<bcp14>NOT RECOMMENDED</bcp14>",
"<bcp14>MAY</bcp14>", and "<bcp14>OPTIONAL</bcp14>" in this document are to be interpreted as
described in BCP 14 <xref target="RFC2119"/> <xref target="RFC8174"/> when, and only when, they
appear in all capitals, as shown here.</t>
      <?line -18?>

</section>
    <section anchor="security-considerations">
      <name>Security Considerations</name>
      <t>TBD</t>
    </section>
    <section anchor="iana-considerations">
      <name>IANA Considerations</name>
      <t>This document has no IANA actions.</t>
    </section>
  </middle>
  <back>
    <references anchor="sec-combined-references">
      <name>References</name>
      <references anchor="sec-normative-references">
        <name>Normative References</name>
        <reference anchor="RFC2119">
          <front>
            <title>Key words for use in RFCs to Indicate Requirement Levels</title>
            <author fullname="S. Bradner" initials="S." surname="Bradner"/>
            <date month="March" year="1997"/>
            <abstract>
              <t>In many standards track documents several words are used to signify the requirements in the specification. These words are often capitalized. This document defines these words as they should be interpreted in IETF documents. This document specifies an Internet Best Current Practices for the Internet Community, and requests discussion and suggestions for improvements.</t>
            </abstract>
          </front>
          <seriesInfo name="BCP" value="14"/>
          <seriesInfo name="RFC" value="2119"/>
          <seriesInfo name="DOI" value="10.17487/RFC2119"/>
        </reference>
        <reference anchor="RFC8174">
          <front>
            <title>Ambiguity of Uppercase vs Lowercase in RFC 2119 Key Words</title>
            <author fullname="B. Leiba" initials="B." surname="Leiba"/>
            <date month="May" year="2017"/>
            <abstract>
              <t>RFC 2119 specifies common key words that may be used in protocol specifications. This document aims to reduce the ambiguity by clarifying that only UPPERCASE usage of the key words have the defined special meanings.</t>
            </abstract>
          </front>
          <seriesInfo name="BCP" value="14"/>
          <seriesInfo name="RFC" value="8174"/>
          <seriesInfo name="DOI" value="10.17487/RFC8174"/>
        </reference>
        <reference anchor="RFC6716">
          <front>
            <title>Definition of the Opus Audio Codec</title>
            <author fullname="JM. Valin" initials="JM." surname="Valin"/>
            <author fullname="K. Vos" initials="K." surname="Vos"/>
            <author fullname="T. Terriberry" initials="T." surname="Terriberry"/>
            <date month="September" year="2012"/>
            <abstract>
              <t>This document defines the Opus interactive speech and audio codec. Opus is designed to handle a wide range of interactive audio applications, including Voice over IP, videoconferencing, in-game chat, and even live, distributed music performances. It scales from low bitrate narrowband speech at 6 kbit/s to very high quality stereo music at 510 kbit/s. Opus uses both Linear Prediction (LP) and the Modified Discrete Cosine Transform (MDCT) to achieve good compression of both speech and music. [STANDARDS-TRACK]</t>
            </abstract>
          </front>
          <seriesInfo name="RFC" value="6716"/>
          <seriesInfo name="DOI" value="10.17487/RFC6716"/>
        </reference>
      </references>
      <references anchor="sec-informative-references">
        <name>Informative References</name>
        <reference anchor="ITU-T.P800">
          <front>
            <title>Methods for subjective determination of transmission quality</title>
            <author>
              <organization>ITU-T</organization>
            </author>
            <date year="1996" month="August"/>
          </front>
          <seriesInfo name="ITU-T" value="Recommendation P.800"/>
        </reference>
        <reference anchor="ITU-R.BS1534-3">
          <front>
            <title>Method for the subjective assessment of intermediate quality level of audio systems</title>
            <author>
              <organization>ITU-R</organization>
            </author>
            <date year="2015" month="October"/>
          </front>
          <seriesInfo name="ITU-R" value="Recommendation BS.1534-3"/>
        </reference>
        <reference anchor="ITU-T.P807">
          <front>
            <title>Subjective test methodology for assessing speech intelligibility</title>
            <author>
              <organization>ITU-T</organization>
            </author>
            <date year="2016" month="February"/>
          </front>
          <seriesInfo name="ITU-T" value="Recommendation P.807"/>
        </reference>
        <reference anchor="ITU-T.P808">
          <front>
            <title>Subjective evaluation of speech quality with a crowdsourcing approach</title>
            <author>
              <organization>ITU-T</organization>
            </author>
            <date year="2021" month="June"/>
          </front>
          <seriesInfo name="ITU-T" value="Recommendation P.808"/>
        </reference>
        <reference anchor="ITU-T.P862" target="https://www.itu.int/rec/T-REC-P.862">
          <front>
            <title>Perceptual evaluation of speech quality (PESQ): An objective method for end-to-end speech quality assessment of narrow-band telephone networks and speech codecs</title>
            <author>
              <organization>ITU-T</organization>
            </author>
            <date year="2001" month="February"/>
          </front>
        </reference>
        <reference anchor="CROWDSOURCED-DRT" target="https://ieeexplore.ieee.org/document/10447869">
          <front>
            <title>Crowdsourced Multilingual Speech Intelligibility Testing</title>
            <author initials="L." surname="Lechler" fullname="L. Lechler">
              <organization/>
            </author>
            <author initials="K." surname="Wojcicki" fullname="K. Wojcicki">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="ICASSP" value="2024"/>
          <seriesInfo name="DOI" value="10.1109/ICASSP48485.2024.10447869"/>
        </reference>
        <reference anchor="LESCHANOWSKY2025" target="https://arxiv.org/abs/2506.01731v1">
          <front>
            <title>Benchmarking Neural Speech Codec Intelligibility with SITool</title>
            <author initials="A." surname="Leschanowsky" fullname="A. Leschanowsky">
              <organization/>
            </author>
            <author initials="K.K." surname="Lakshminarayana" fullname="K.K. Lakshminarayana">
              <organization/>
            </author>
            <author initials="A." surname="Rajasekhar" fullname="A. Rajasekhar">
              <organization/>
            </author>
            <author initials="L." surname="Behringer" fullname="L. Behringer">
              <organization/>
            </author>
            <author initials="I." surname="Kilinc" fullname="I. Kilinc">
              <organization/>
            </author>
            <author initials="G." surname="Fuchs" fullname="G. Fuchs">
              <organization/>
            </author>
            <author initials="E.A.P." surname="Habets" fullname="E.A.P. Habets">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="INTERSPEECH" value="2025"/>
          <seriesInfo name="DOI" value="10.48550/arXiv.2506.01731"/>
        </reference>
        <reference anchor="CROWDSOURCED-MUSHRA" target="https://arxiv.org/abs/2506.00950">
          <front>
            <title>Crowdsourcing MUSHRA Tests in the Age of Generative Speech Technologies: A Comparative Analysis of Subjective and Objective Testing Methods</title>
            <author initials="L." surname="Lechler" fullname="L. Lechler">
              <organization/>
            </author>
            <author initials="C." surname="Moradi" fullname="C. Moradi">
              <organization/>
            </author>
            <author initials="I." surname="Balic" fullname="I. Balic">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="INTERSPEECH" value="2025"/>
        </reference>
        <reference anchor="COOPER2023" target="https://www.isca-archive.org/interspeech_2023/cooper23_interspeech.pdf">
          <front>
            <title>Investigating Range-Equalizing Bias in Mean Opinion Score Ratings of Synthesized Speech</title>
            <author initials="E." surname="Cooper" fullname="E. Cooper">
              <organization/>
            </author>
            <author initials="J." surname="Yamagishi" fullname="J. Yamagishi">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="INTERSPEECH" value="2023"/>
          <seriesInfo name="pages" value="1104--1108"/>
        </reference>
        <reference anchor="DRT-REPO" target="https://github.com/cisco/multilingual-speech-testing/tree/main/speech-intelligibility-DRT">
          <front>
            <title>Multilingual Speech Testing - Speech Intelligibility DRT</title>
            <author>
              <organization>Cisco Systems</organization>
            </author>
            <date>n.d.</date>
          </front>
        </reference>
        <reference anchor="MULLER2024" target="https://www.isca-archive.org/interspeech_2024/muller24c_interspeech.pdf">
          <front>
            <title>Speech quality evaluation of neural audio codecs</title>
            <author initials="T." surname="Muller" fullname="T. Muller">
              <organization/>
            </author>
            <author initials="S." surname="Ragot" fullname="S. Ragot">
              <organization/>
            </author>
            <author initials="L." surname="Gros" fullname="L. Gros">
              <organization/>
            </author>
            <author initials="P." surname="Philippe" fullname="P. Philippe">
              <organization/>
            </author>
            <author initials="P." surname="Scalart" fullname="P. Scalart">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="INTERSPEECH" value="2024"/>
          <seriesInfo name="pages" value="1760--1764"/>
        </reference>
        <reference anchor="URGENT2025">
          <front>
            <title>Interspeech 2025 URGENT Speech Enhancement Challenge</title>
            <author initials="K." surname="Saijo" fullname="K. Saijo">
              <organization/>
            </author>
            <author initials="W." surname="Zhang" fullname="W. Zhang">
              <organization/>
            </author>
            <author initials="S." surname="Cornell" fullname="S. Cornell">
              <organization/>
            </author>
            <author initials="R." surname="Scheibler" fullname="R. Scheibler">
              <organization/>
            </author>
            <author initials="C." surname="Li" fullname="C. Li">
              <organization/>
            </author>
            <author initials="Z." surname="Ni" fullname="Z. Ni">
              <organization/>
            </author>
            <author initials="A." surname="Kumar" fullname="A. Kumar">
              <organization/>
            </author>
            <author initials="M." surname="Sach" fullname="M. Sach">
              <organization/>
            </author>
            <author initials="Y." surname="Fu" fullname="Y. Fu">
              <organization/>
            </author>
            <author initials="W." surname="Wang" fullname="W. Wang">
              <organization/>
            </author>
            <author initials="T." surname="Fingscheidt" fullname="T. Fingscheidt">
              <organization/>
            </author>
            <author initials="S." surname="Watanabe" fullname="S. Watanabe">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="INTERSPEECH" value="2025"/>
          <seriesInfo name="target" value="https://arxiv.org/abs/2505.23212"/>
        </reference>
        <reference anchor="UNI-VERSA">
          <front>
            <title>Uni-VERSA: Versatile Speech Assessment with a Unified Network</title>
            <author initials="J." surname="Shi" fullname="J. Shi">
              <organization/>
            </author>
            <author initials="H.J." surname="Shim" fullname="H.J. Shim">
              <organization/>
            </author>
            <author initials="S." surname="Watanabe" fullname="S. Watanabe">
              <organization/>
            </author>
            <date year="2025"/>
          </front>
          <seriesInfo name="DOI" value="10.48550/arXiv.2505.20741"/>
          <seriesInfo name="target" value="https://arxiv.org/abs/2505.20741"/>
        </reference>
        <reference anchor="DEMIRSAHIN2020" target="https://www.aclweb.org/anthology/2020.lrec-1.804">
          <front>
            <title>Crowdsourced high-quality UK and Ireland English Dialect speech data set.</title>
            <author initials="I." surname="Demirsahin" fullname="I. Demirsahin">
              <organization/>
            </author>
            <author initials="O." surname="Kjartansson" fullname="O. Kjartansson">
              <organization/>
            </author>
            <author initials="A." surname="Gutkin" fullname="A. Gutkin">
              <organization/>
            </author>
            <author initials="C." surname="Rivera" fullname="C. Rivera">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="LREC" value="2020"/>
          <seriesInfo name="pages" value="6532--6541"/>
          <seriesInfo name="ISBN" value="979-10-95546-34-4"/>
        </reference>
        <reference anchor="SCOREQ" target="https://proceedings.neurips.cc/paper_files/paper/2024/file/bece7e02455a628b770e49fcfa791147-Paper-Conference.pdf">
          <front>
            <title>SCOREQ: Speech Quality Assessment with Contrastive Regression</title>
            <author initials="A." surname="Ragano" fullname="A. Ragano">
              <organization/>
            </author>
            <author initials="J." surname="Skoglund" fullname="J. Skoglund">
              <organization/>
            </author>
            <author initials="A." surname="Hines" fullname="A. Hines">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="NeurIPS" value="2024"/>
          <seriesInfo name="pages" value="105702--105729"/>
        </reference>
        <reference anchor="UTMOS" target="https://www.isca-archive.org/interspeech_2022/saeki22c_interspeech.pdf">
          <front>
            <title>UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022</title>
            <author initials="T." surname="Saeki" fullname="T. Saeki">
              <organization/>
            </author>
            <author initials="D." surname="Xin" fullname="D. Xin">
              <organization/>
            </author>
            <author initials="W." surname="Nakata" fullname="W. Nakata">
              <organization/>
            </author>
            <author initials="T." surname="Koriyama" fullname="T. Koriyama">
              <organization/>
            </author>
            <author initials="S." surname="Takamichi" fullname="S. Takamichi">
              <organization/>
            </author>
            <author initials="H." surname="Saruwatari" fullname="H. Saruwatari">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="INTERSPEECH" value="2022"/>
          <seriesInfo name="pages" value="4521--4525"/>
        </reference>
      </references>
    </references>
  </back>
  <!-- ##markdown-source:
H4sIAAAAAAAAA51c6XLbSJL+z6fAqmOi5R0SImXqjImZpQ63tdY1otyenomJ
iSJQJNECAQ4OyWy332WfZZ9sv8ysAgogqW5v/2hLAKqQleeXB9Tr9TpFVMT6
1Nt51Hnhnami0NnKm6aZd7csc+/m2jtPQx14l58LneRRmuQ7HTWZZPoZa25i
vklLzcqdTqAKPUuz1akXJdO00wnTIFELvCHM1LToxTqYxzrrLeKAlvYKrO1N
ZHGv3+/k5WQR5fSiYrXEqqvLx3ee952n4jzFG6Mk1EuN/yXFTtfb0WFUpFmk
YvrlanSGf0D5ztXD47udTlIuJjo77YSg6LQTgHScoMxPvSIrdQf0v+2oTCvs
Olou4wiE0/E8lYTeg1Zx7zFa6J3OS5o9zbK0XNJ5VTCPEu1da5UlUTJjPo3K
MEqJS7iw03nSK6wITztez1uU+TxT9FOYFfSPflZxya/pPOukBFWe9w17e57w
ZOcTaKJHfqC1dH2hohjXDVP/K9LF1E8zXqKyYI5b86JY5qd7e/QkXYqetW8f
26MLe5Msfcn1ntljb6fTUWUxTzM6CvbxvGkZxyLKnWtVZgqksix3+C72UUn0
C5/u1DuP8iD1xqu80Iuc72tDY2w04L8CesQP0oWsD9IyKUhtdj4mUaFD7wNO
GPLdtdd/UIso9j6lPwdR8BR92/ufaO1L+vP294/KvMhUHCmwIEmzBbZ8ZlE9
vDvfHwxOzI/Hg6Oh+fHwaHB42umQwjuPXz1+7D3698f9/im/wVrajQZbw5wF
DHX/WQe0wgs1bGARJXwCL51CTVWSG2Pw/l2ComIltFaC8czJT+VlfIHV3Ruc
nBz2+sd8JddZpHOizi7hp0+h5jj+AuYk77z3Qash/ME/Gw8O3g57bzcRz7QX
c+3Sr/Jc5zl2K4j4KKHTwD5BjSXei/WzjumuYr3ORTyvnunBOdN+f3DQG/Rf
OdPD2pnOxr4cwxXIUfNM4/oQ5I68BZ8xjdOZeEI5GRlcvtTQXj5cHEezaBJ9
k1BwAAhl/5uFcuQSf7yV+Nq9EI8NrZb3L1Ex95QXwMrDPC2zgM6jlsssheP5
hhPsD3r9w28+wbFzgsP95gnudRboZQE6Xz/B7v3l+K9vTr0RblZnXtT6iBf2
irSHf9orm6qZqAxMQNDBg5CjXs5T+N1EF+Tqxf+b9ewJN+pnbxt7+oNtAi5U
NtPFqWc98cvLix8VpQ9l2svgcB97D5fnPTDrkNafP9x9uhjffXw4v7zoXTw8
Nll2XkkRjvKmjAvoYTIjDo6F8qumhnoUok0MWT+KeNVr3zr01o0PfuVqt8n9
fDQe37N2DM2li7srOKG+Pxj0T/bk/vB4eHzg0zP+oD8cHh0fnmxkTKS1/ryM
04wilNYcoYAiSpLfnrPy+nJ8/n50e/dp/OEn7HrQZNGZToL5QkmgvNWIVxVz
BNG0WcQGMr56TNP4NTaNiE15MFcJwuXTao1X4Na1esrn5MkztVKJWt/gQf2s
cv00V21WQwZnep6B5DUpXPmIiJBy0Lr+g++9K4N53rp86Y/8e997rya6yLeJ
7fbx8mF8f3l5/p5ld9CSHcR10Ac4+Fv07O8f9A/9/uDo7WCjzFT2GQ8xlpjk
e/XDz4O2Lt98HL9/GG1TZxKWPMEqm8PVcpwZzTSZ7g860RkHWCvLR/wvIVeN
s4G3kO1iqcwjo0TFqzzKaaXjJsnA76rfjGV4Jir/vyzk3Pdu0kyF0brMzuCB
gm/i/+9hbf/kgMLg+d3d/eUDVraC9FXyTKeaKT7Zg4I69S7ZGf5CF84ixYy9
0SoBzI8S8rfjAAaHZ2mJcGyVgPN59AtcjDD7Nd5c+mB9ulxjzX/73k9qoWZR
Pt/qPVpseGuuL9WMZAoPMuz18P/j7W40D1TPolriFKMPceL/oh33AqZt/+2/
nDv+MpxiSzhXeN77uxbO2eBUrar0tnlZbLU1kq4j0vZJZnBA5YQQ6R5j072F
Q0NPaOZ8CVf2ikxrAvPJnrnRwiQUM/CWm4/X16whwxZqaAbIZtxNxFsKSNsQ
A72WjB99ikHrZjEmTzdLi3UzQuLSdlhwVvdzkL5c6vU740Ahayl+pwINWwp0
dNiHAh0dDhts3/kWDRqSMHDC/WHQViFizceHHy5vH9eD0FX9KBu4edDqz2WC
MBJoBibnc4UXwFJfZTWiy1hFP6ety5987+/YarYugPM0S6AXrRsPxNK5jiYb
fdl124/93fdu29cQxj6Ui7UIdkMEBvPW1Z8oSK3T/GmdZOjSO/JARF3Y1pwx
LSkQUif6G0PabzpVAJO3+wNCXh9vr3o/YpNWjEJWai57P0KmMJa4CkKjGl8a
mI2npxEc562gyldlChc5nrfZ+96Xy4vfYEEFyw+2cGRLMCcgdjQcfAN7zOMX
lzdX4ML7q1u8tP8KLJ1Hs3nPOpiPHzjsXmU6pn8vk1mMiOBdRCpGHLZ4G4dR
OELhv8ovRNULvYggg3mUtO7dQSt/hqdA5pyn7ZtQ2R/K4mltEVT+AXafqS0s
vAYwZx73m45l5/Dg7X6vd3gwHOxY/Ruf3Z56J0cnyFN7JwcHw8Meks+m43H9
jgriFz0RTiPccs65R2/yY+QEvQFSJ1o8Pr97uPxry4XLNauDfzWcbuvieZoU
mcoZ6zzoWaa5nvAbCBduG/h2g54+pbO4TML1Fe+jRG/DmYS+r+7HG31z/+Co
Dx7Sv/ub0wFkqIHWVAHLfYpM0TL3g2BvqRDQ/zWFFeby8x67abqwN9GBPtL4
9eBAHe4fT46O+np4Mg2m6uhkMBge9e5pQQ+smeoMeYI2UODj483duGX2fAl3
0qdV2hurrLxWExPGOev8MY0CjWdqB07H3H+NwY/kI/VT2+QvfO9va7oJL3mr
nmAX63t8SLNoBWy17iEeFVW5gg1OxaMTvGC77Peisf2mwIYH+4NeD//fDFZ/
Tyzd38vp9Pv7a6G04/t+p9Pr9Ty4HWhtUHQ6j3MAeJv+eVCGZQoFb9RoyKOw
6+AqQAPKLExVNbZV1d2b6zeCaTxdlbS7nRwplAdUTLlGqPXSgJ9Mh9B2xOiV
t3vxcHnxpstGZZISrpDLXrumCvgG9PMBFlEYxrrT+Y5AYpaGZcBlXxxHS017
jTKuNFnIxaSaUuwb78XUe7lW7KloAUJTr0PltAxi8YIyy4g7xIhpWZQA8uly
mWZFmUQF5Au26RwPwCtPVlTTF1DndbCLFgzSPtCXL+ZEX7/ythFyMYdhXgeZ
aFwyndgkWkAwcDB13YRuVOUXrM/SSZkX8BFM+lIFT7rw4jTPfa9zOZ1urGC1
xeTtWjGJLECDN0nh43L4+1DFVMbhd9VSQzDhhAbP5lz9KgOEHG8K1QK7sZ6k
oJ/5GHMoVl1Zyv3OFQJTyXzMseeSpWihPqva50gSAofEzu6UNyaSAt0lfmsE
a0LxHg4XhdXpXuZQYQ2ZY2/oesRKsIyWOia1WKgCECj3Jio3F8CM2IPb4hoz
9n7TobPy5vScW+7A7kYNjGDIdnD+mSLCSBueo7TMY9AGOicUikk1zKtw8s77
9IWUq0vMTsuCLofErryc5PrfJSlbrlmlSRf0FgHO6nQdSscnqHQRrEpg09Bo
qJx1nVJ9S+C+EaUKQJucVIpExmmoU23mw6ebSp/iGUivtrgOtwja3JSrv6YZ
1e2QQs0MoAOaCKiUHcoz5G2gWjg7lFmFYcZ6DQuCwYE7kOi/yygzbO+wZQcw
UqqFi4JaZYpd0jntrqzTZ19hqA4btOEJCCc3yt+qdZKxNnPBprXaskoAz5N0
vSSNcjwhZGHXCSQGbgVpEkZWvrJnkGawBRZKRv0xeKU4NC6LGMIc1w3PbD1U
3mQ5FK4g1cjhZtqFTvgbcUuk116mllEoJVlKAelSXcwl0l7mhMMDAjlpHIsd
a1haFiljqY7sWJZJmvSwU4/UPQS1Ki+NpKD3V+LZQfQ2/cU7M21gg9mjUY3O
IjhW+BdNesEchL+JowWps2Gn0Ex0QFBZmdd1bKg7LsxmuM20CgGxaWt1Pe3P
/C6YVuebxj23+CgltK9fIZJz6nuyxcYQM+tozrW0ObK3pBYF6KKbsxQyFX+a
scWzjr2iXI48SM9Yh5O0YFHB+6QZO0JRZoefiZIw5Sg893hpTUwhq5irYku/
xesEaQkyJwg40YwyrUDR8ay3Y4XZg9GFwLsqlKgnoSOeATQV8wW5h3fswhXe
Bz8t3FhKK4LcCHfn0gUskNoD+Nc0tom26vUmeIauIk5LiQQL5D8FPRQZAKCl
TZGS129GUHJ/8AZJ7QXY2jJKjczrEX2fNcJVwXmVR/aPRUJwREc2IvI745Ij
cE2HWpFAmFyVR2ATdRqDQhy6lSy7BRBFu8dEObeH2Flz5yvRhnx+AHtEapak
CH+BpbBnO//YhXEYyLW3uPHXfUV1Kq8tCGlDmLFxg1XY8TFd70mTQ0T8DVm5
apc1LTOOsIsUKifBAbhAZ/HK+omN3pVWi0u3j7XiU8O7h4C7ZU57qAmkQvjv
u++8azYrWs9zFaa4TLe+s/XtwbhT/eTBbthnSYeK3vkCTvXc8Gwe3uWqIHQW
Vhrh50g8xRx4EwyvXBMTCt1E+vHmlYamFf+XL82mL9yKDRHsqDaHBiK7YiG3
hE2FlF7eRiR1YLNHweop0niyXBWqJUfXtBWeu/x6/RmKXlTOiqEKY5NQ50EW
TUSXtzrBe5VBUctYQfRMp+OIrFeH/cJuyFgCRR6yaZNdhmlNtx9XMmbtYV7Q
I+z6mj4aeZcmtpELJpQqnW/Rxmq1SIUUCUgeCehCd237g+IbQCa3zsGzzzrc
KGlkfnOKO88QE0dd0iUjEue11f6eyrSBLRyhsC1D2dDGTrPeFbmT/1jlyajB
wLQxFcZGDHng3HMasZuzpxC4yNSxM9yyfY6clV/vbBFG0ylJkrMaN7LPNSG/
bEYPwXTxBO5DkYDLKoMxPo7RBIeZaeUIAOeeqqGGJatLtFRk30SgRvyoSKiU
cArVmJU44NJVryjZpLFQ+lHOMahb2z90LCdKaCJH8jQ2ZdgAUYEUOI3Logo8
xGdpzuyOzh/e2NOI3iIAkoMvc7EhYCWih+AmIRyAcdPlyanLgx1u7sZvjHZB
GyLWP+aJqJMJFFYZKEBKfOMQPdHEhwX1i3IJ9Ngl8UBV1wuBfs0BjJ5a5SCO
cNQRxGiUBxlSWiYcofnQGzTI9x4qy4sJaFO1xRxkrmDCEw3hh3oB/SkyZdxI
XkLLIKQsXQgNPV13wCaRokwADqPqoH39Krm9UcmccYGbOW/STLx1KokbXW4d
1cn0FJsQDLEoCT7vzNI03OFRtYnCD00IL1YcZuplgjTZxoNaa1gtGA4G3Oy0
brOgDqQBnOZpsZ5IcpOc4zrH/wAAnlSFJuKw5JkRQOo9U26RLwhYkKEJzym3
LF6IxUZVbax2fWaaYN+JNtBCwB9kEjGEETKjHBwA7OIYeHH+wIFeEJpwqqXn
3u4FqbkdUaFTWPWW7ImU212/rujYghTdRDeZxSJnPIpxgBIwShS/WC2BIGOc
gF9gSviN+rXjzthdWJdN5hCoJR+YHGAVWJsH430nKwt02VvRHpQ8gPAo5skV
CkWkv4JkVZSxm68yLS5xfflSt/ToKOzwHRaJVRqxGsP8LU+yxp6EUTgDnHoa
zR6b67EhgXfyl428zuSW4MszjRkaLXC2P8b2RgEeHkUBahyZzVfWrHZxu0HX
EQCJSdXybVkB+Gusld0T9ACa7BEfmeMm1AWmHg5yK2gtys7uUGeLXMIsFx4Q
bLpVdmmXELGWlSIBbtJGfAqOxkgQ+JcIcqAswPAPpzJORhSIWsGwGqozmEJa
ELMK0YKQi0ucV3ac/aeaM6fcGB9FcPs6VmEuqYIPhbhtW/KABk7LGK9IY5Yb
p3VSYFN2TKKN2uRsVtenmdbiTk0c79k4TusWcL9QLPcSRTcxI9nDcTa+d1UY
twTLIpVluJuXjEEYom6uGHDwkKBwphUNypDUIOvQ4HlwLGLH3VTOOnDXNRAN
HU9XFjq6OuoY+QsdwoDTNX1v77iOQaWgIZNm2O1ZU92RH5ySecaKmvuE1F5I
P5elBftfvtixBGvkVd2JSYJf5YwetKVLU9f98qU9EcUFADa35oTNiA4kwLZz
/tqBoA0IcIWJohEUnsp1gNxanqKDWRdprASiTarSh4UC1pY2+BVba+ACHx/S
urM1Ia4T2G0JjnWOagARZf1G52E5DNxaysFwo2IEkQKPRfUt6LFB+MIEMksa
DtbUkHBzQU8SHwNdgnryiCJJ4niNph7q5DnK0oS44HvvGtViHG7Jxs9MnZqZ
dNEfWyJmiw+5qiGmwwcJeH5S7MzkhfRgDV9oj4CKytgpQtzi21KCEBNg+yH3
YEE/8u+cdYQr4eQBTD6d2flOgzw46DUyAmzhgujvCZIvqRBVgXKLLxBPgFOp
pg1h2vL+aKF+SWk+icbsyK95j2X21PXuwU9xyVz95ULK9qzvkkKi3d14I1Of
zTX75GkUU/9JQHALQuOQGdyREwjcIzGnxDCrN0getqp25Z3yOdeKVPyiVmRP
VPtNMxZIXV5mdpushdPaKmLbVIYwlnz5QIBBWhW2tyH+lx6QMFbVQiaMyQS7
VVxmj0CcFOVtmH+ramG+6aiqohAt4RNEn267au0OzDpagN1gawH5jDQms0ht
UaFO+UBQ1W2YRVQ14vIHTlAD6mVaUFinoST3Uw0yYwZNSXNi19R/nII8cx9K
LkGzLjkS2yAUxWFLbJnqMOTl6FsLPpsLwcAgSE96hgIebY2zaJpClLlFX5Pp
U92f7VFKD87GOdw5vbdZF7S+phm9HNaxaFxnK8x1xim62+tclaLkblup0cJJ
uNAYg+n19Hu30Sfgfg21EKq7+Fc9WTUkmgOpGzdqgW3wJqZk+5jU3GDgDJeK
vEHSsDClkVoOenZ8xI2pSCRLKQ5W3TjuTIqHq4sAdciNuHwUcb3G74yp1mtw
GnSqjE35ZN5sC9XdMMJtL045zY6XO81Iq2cN/pKfMWm0uIhdAadGs0EMMvNm
lY5F6GzBSln3OBEqCcWKjjmtVbeusiutA+rw/pG7p9CdERWQG5ph8clLhFsT
6kKGBpUEhDUqVK8KmwcvuefCnS1QNTz2nt7/whC/PZdyWX/7JIiENdWZrPR+
lJd3zlYGmtmj16lvlemQQpU0HMnpfCNV22ICG0tuRtdt0kdRihJWg1pskb2R
lzt1txgKkBvPaOBp7QNYxIILuJDPrW+jUA6cZyxNdpQzBLYzCOwnsudG9lUZ
1lwREtEZ75Az4Cqo1SnZfisOa1X5Ze53WNtwC47U2K21ljyqqUHQVFSuLXwv
HIMc9PtURQUzgZViboI0Oc+TOV3pVJKMaIGNvnIQU4ozFppXJRcGm5yfKJih
uJhgjhwZbP/eZn80UMhohBJeI2EJ6g1FNo1SVR2lrlUGAdfv8aNxJyYz5W//
PrFXvnCS+IaKjvJmwLFxxOb5SuIXhRItYnDjVpfqDnADtiuTUFobCgphMMcF
soI9BHNcpORWkBqstlUpbMRHC7NV5Zd1j/O2hoXw/usp2JZIJkf8nnKOHAbH
qs/4qI6KNZtMW02As0m6XXlwva4qblQ1wIvzBxA+ZVElXj1OgLBN7zKZvzkl
KZRharJyu6xuzcVpulX4HMcwPeo6mTJRivXgpN+aDHA0WdLMONxSm6nit7Q/
wfdmR114c8eGTK2L4YHZ1oB0J7gxXsqlMuRGWkMNbYvVqSt9fqF1T84bay2Q
T+/kdXQM5qopN9kccUxpA+SvCf08qph8TUPrb1LJzqwiIzJkUl/IDLZz1YXL
YlUbwTg0hhHOe2gZTUZkztd1bP910Z4JgZerQCIOvCQHGEk1LsotdFcNwEQl
fd8btds0ZRVRSenczphRQFOzoR/AJDulUzllE3/XcRFNcoWcLTp62KiR0kGt
sXLGQCoMNEnTSDl5Z7MRDR2ZOU2TYjSCtDjiWm33+0aXeGQqeZbpZAr6FvwX
L6nENHIFFUNF6veCF64JL4ytFnceatV2AQXN5EkTlNrrLEiOnnxzyQiJsEaN
YGtuu9uYhdiiytWNkTYn3aKEAcubKjfjiC/5mQ3dmyJ1CyDZpMCmlzzmRqZB
QKh6ZZHOWNTEmAtdAOrACs4uGmCm/cHH7wc1j1L+a4wobv2gtLu95NToTW0q
NnUbTVkJIgY7ik9R0l2vipK2XFCJgqtY7fKTWePqHWtVA4EzelCmh0S/TDX/
KipnAryAW4Yl9KU8F2irEGYHT4hVfJeslxdYzN8sCvvCZRte6MEpOGWaODXW
56IJIbJGZbrq5QRUpwlMALuQFlbVuvvejoVQhaXVMTHfQBfmzGY4Ik4DM7EW
lHaURcq5BPvmklQQ93h6tqpDd2sYJ0KMFrBGAt5VamCpbn0oW4M3q+uMZ+um
rmm9sQHRt47UgAC0mHAmw8869lmNObRTDDIN1vNbzvgaCg4QCnkJBK5sU20B
DxKT3Hr7NkNoZK/OqSfOX6qo03lC0zTjaFJSU7Yjm6lrfg3o1AAnjWEC4/B5
UJKsTsGnLwE7xbh7+VwttQ2uu+Px7ZuqQcWFplwyf5VZhEKjBohI0maQoNBu
YBgvVyUlvveJm67t2CMvpZBCIUfmaZ0ONCMkowlOxYJRPiW5duJB21qLuzfX
4U2wrs5HhM11vOTg1tLj9iFosswFm2bqU2yg7tGSamj+uM3AvXrexC0NPdbz
ag5Hjfogf8tzUxey0iXHweODDBiRcHCgrSYOjM3PwZIe6Z5jqo6Rk9VKabby
ENQwpXBOF0S5bJHCLRLYluBMm7bsC5xZ/nss1gBTKBLX982AipnKZb9N5UKm
uadkqDJ0z8GJq0wtqk0Z+1pWzDACfr7xOQ83DLilaCbLZdtywUTV2i0lThEq
3SHYkcbP1pdAvQthk+6aEqbp1OB8xol4lzJ6h3yLS5wdQa9S7sTBXlhhH5n4
gSm2cJMknZCJC2Qw8KL9RxtqQ5aZLK/1Fx5MuyUrE6sRtrLcKtXbqqmBYW7s
lR1kEszG2UTqeja/dXJL45Cp9EMlDOhCVTMBCyEbnODk+A9SQ7YXBoN+7YLq
P4wDXbmjdnuTnIha2gWnRLmOp72qzjy3fTJOW8wkeBLWKyA9+qhZZdydoVSD
zxYlz5GZpajeRMnCqEq4aJppc/82L6hNZ5jEswHpUieNrZjoedoillJiPMVp
Iv8vXOWx/hwpwLBfzYf7v1Y1JShIA3HtSqXojfePk4M/eOdX/8Szbci2toY8
Bn/87C7r/Hras/85P/ZOm78QTbxZlCzByl8hQ/+t948/7vW8vr9Prz8Z+idy
YeC/5Y0Ffj4P/AN/3zt5mixz7/buenR+iaePD/yhffqIl/f9vlzY9/vblpvF
R3il+ywtPrS7HW9b/Kt3cFivIxK945NXX3po1u0P7WN9Ifb40B5+3x/KOkbY
45G3b9YcVuyw7zq2J15bM7D07dfkDHnN0D+wF46ba/r+gaUORNmHWBJHA3vh
rawiN+T8IQAXy9+tTXQrHlamfgZ7wbDMbGITUmadLqvyb6sRwXZvpsHcpglF
uTIo2lGCgPN0bV/4Jh2s4MPhPAUT1DUSjnzO6M/F+N60Lysa3Ntrc6XtsUZz
Yiyz+RL93RUHdx/uU5ZhmjuIL1wuMp83xKsa/LlDlna6t9nu5I2pMccTXHqx
jDIzxSABwHoCZQZ3qR+5MPiP/oZBZ+Msp5Nyix/hgemyIAu1qIR6BbRYHuXJ
d/MELZOxHreNZlprXJooGlXg7ZP3nVFh+69dsyjX9F2cVBYUhdBENbSMmqwx
gg/9tQSolyiPbUhFSZI+N/hpJvEsPu56tcyaXxdc3N42pcv9AP6SgL5v/Pr1
DcRtb1VfScg3SI6GVG2EL1/k61NZSDlqugAJFPVl8Kpqnrnvk08X7OfVWFtp
s4wzSKk8tgW86m9YcMl006cPXVM+SxEZwQjqnUlNMwUCZCzsdICYMxWjTclc
WKdi6hjmZhxspQsTz3NSS6NNZiFBYRfGcp2axvmMCZhmV/PDDf6iRbnl4HWl
bc+IUCXV9NudipRQTX+2zngkpzVKdifugL+aoSMF/CHVxGSnQE3wQ0m+/qGW
M5vdeS9jbi9V3iVMtNLQhiHrH70QQbVb6lBVM5LEimYrbIekngSwkL3Vk4Qi
JebjHu7ycMPGTLm6nU8473OqdiX1n+274GlMmWthPEmfBlAZIfd2YJiP9FcC
6V8EXP4ZGvzxCpGDfh6/H11fVz90zBPj93cfry/qn+qV53c3N5e3F7IYV73G
pc7OzeinHWlw7NzdP17d3Y6udyoBVh+l8QB2ar4SkbktxtidRi56dn7/v/8z
GEL7/8P8DbqvX80v9Ffo8Asl9GYqgtCW/ArmrjrAmwztqOkYU1E2KlQsUwbi
ZKlID3b+5z+IM/889f40CZaD4Z/NBTpw46LlWeMi82z9ytpiYeKGSxteU3Gz
cb3F6Sa9o58av1u+Oxf/9Bf+mLI3OP7LnwUAjDXMlkLCeeMTEujP2QU/cDW6
Ha3fbEhxzg5XnlRBpaD0HTCN8Hb+D1yS3LtoUwAA

-->

</rfc>
