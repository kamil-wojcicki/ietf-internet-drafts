<?xml version='1.0' encoding='utf-8'?>
<!DOCTYPE rfc [
  <!ENTITY nbsp    "&#160;">
  <!ENTITY zwsp   "&#8203;">
  <!ENTITY nbhy   "&#8209;">
  <!ENTITY wj     "&#8288;">
]>
<?xml-stylesheet type="text/xsl" href="rfc2629.xslt" ?>
<!-- generated by https://github.com/cabo/kramdown-rfc version 1.7.29 (Ruby 3.2.3) -->
<rfc xmlns:xi="http://www.w3.org/2001/XInclude" ipr="trust200902" docName="draft-lechler-mlcodec-test-battery-00" category="info" consensus="true" submissionType="IETF" tocInclude="true" sortRefs="true" symRefs="true" version="3">
  <!-- xml2rfc v2v3 conversion 3.29.0 -->
  <front>
    <title abbrev="MlCodecTestBattery">Test Battery for Opus ML Codec Extensions</title>
    <seriesInfo name="Internet-Draft" value="draft-lechler-mlcodec-test-battery-00"/>
    <author fullname="Laura Lechler">
      <organization>Cisco Systems</organization>
      <address>
        <email>llechler@cisco.com</email>
      </address>
    </author>
    <author fullname="Kamil Wojcicki">
      <organization>Cisco Systems</organization>
      <address>
        <email>kamilwoj@cisco.com</email>
      </address>
    </author>
    <date year="2025" month="July" day="08"/>
    <area>Applications and Real-Time</area>
    <workgroup>Machine Learning for Audio Coding</workgroup>
    <keyword>mushra</keyword>
    <keyword>drt</keyword>
    <keyword>evaluation</keyword>
    <abstract>
      <?line 198?>

<t>This document proposes methodology and data for evaluation of machine learning (ML) codec extensions,
such as the deep audio redundancy (DRED), within the Opus codec (RFC6716).</t>
    </abstract>
    <note removeInRFC="true">
      <name>About This Document</name>
      <t>
        Status information for this document may be found at <eref target="https://datatracker.ietf.org/doc/draft-lechler-mlcodec-test-battery/"/>.
      </t>
      <t>
        Discussion of this document takes place on the
        Machine Learning for Audio Coding Working Group mailing list (<eref target="mailto:mlcodec@ietf.org"/>),
        which is archived at <eref target="https://mailarchive.ietf.org/arch/browse/mlcodec/"/>.
        Subscribe at <eref target="https://www.ietf.org/mailman/listinfo/mlcodec/"/>.
      </t>
    </note>
  </front>
  <middle>
    <?line 204?>

<section anchor="introduction">
      <name>Introduction</name>
      <t>The IETF machine learning for audio coding (mlcodec) working group aims to 
leverage current and future opportunities presented by ML codecs 
to enhance the Opus codec <xref target="RFC6716"/> and its extensions, 
including to improve speech coding quality and robustness to packet loss. 
Effective evaluation of codec extensions (such as DRED),
in both standalone and redundancy settings,
is a crucial factor in achieving those objectives.
It supports reproducibility for existing extensions 
(for instance, by enabling validation of whether a retraining pipeline matches baseline model performance)
and enables benchmarking of future improvements against previously established baselines.</t>
      <t>However, as outlined in subsequent sections, 
effective evaluation of generative ML models presents 
numerous challenges and necessitates specialized subjective 
and objective evaluation methods. 
This document proposes a crowdsourced subjective test battery,
along with associated test datasets, to address the unique requirements 
for accurate and reproducible evaluations of ML codecs.
The proposed test battery covers both speech quality and intelligibility, 
including tests in clean, noisy, and reverberant conditions, 
and incorporates real-world audio data. 
The methodology leverages crowdsourced listeners <xref target="CROWDSOURCED-DRT"/> 
to enable rapid and scalable assessments, 
while controlling the variability associated with non-lab-based measurements.</t>
      <t>In the era of generative ML models, 
reference-based objective metrics face additional limitations, 
while non-intrusive methods struggle with generalization, e.g., <xref target="URGENT2025"/> and <xref target="CROWDSOURCED-MUSHRA"/>. 
Consequently, the use of human listeners, 
the gold standard in both quality and intelligibility assessment, 
is of notable importance.
The generative nature of ML codecs also implies that speech intelligibility 
could be significantly improved and/or degraded by such algorithms. 
For example, human perception for some phoneme categories could be enhanced, 
while confusions might be introduced for others, 
including hallucinations of incorrect phonemes even at high overall perceived quality.
Such confusions may not be easily detected in quality tests, 
highlighting a pressing need for highly diagnostic phoneme-category, 
or even phoneme-level, intelligibility assessment methods.</t>
      <t>The subsequent sections present the methodology, key considerations, 
and further motivation underlying the proposed test battery, 
addressing the challenges and requirements discussed above.</t>
      <section anchor="listening-test-methods">
        <name>Listening Test Methods</name>
        <section anchor="mushra-1s">
          <name>MUSHRA 1S</name>
          <t>MUSHRA 1S is variant of the well-established MUSHRA (multiple stimuli with hidden reference and anchor) methodology for assessing quality <xref target="ITU-R.BS1534-3"/> in clean non-reverberant conditions is proposed for testing and benchmarking of ML codecs. MUSHRA is firstly adapted to a crowdsourced, non-expert listener base, as described in <xref target="CROWDSOURCED-MUSHRA"/>. Particularly for generative models, which may cause hallucinations, a reference-based listening test is preferable <xref target="URGENT2025"/>. Secondly, one system under test is assessed at a time, in the context of a fixed reference and anchor. The advantages of testing one system at a time are the unlimited extendability of test conditions within the quality range of anchor and reference, avoiding context effects of other conditions within the same test, avoiding difficulties associated when merging results across multiple tests, and simplifying the task for the participants thereby avoiding listener fatigue, particularly in non-expert listeners. As such, MUSHRA 1S has similarities with to the absolute category ratings (ACR) tests, which can be used to calculate a mean opinion scores (MOS), in that it is simple and easily extendable, while also being more stable than ACR, due to the fixed range of expected audio quality, bound by the anchor and reference. Reference-less MOS scores have been demonstrated to suffer from range-equalizing biases <xref target="COOPER2023"/>, with other samples presented within the same test defining the range of expectation of what constitutes "good" or "bad" speech quality. The drawback of the MUSHRA 1S solution, compared to a traditional MUSHRA test, is the slightly decreased sensitivity to very small differences between similar methods, which may only be detectable in direct comparisons.</t>
        </section>
        <section anchor="dcr">
          <name>DCR</name>
          <t>The degradation category rating (DCR) approach is used to produce a degradation mean opinion score (DMOS) <xref target="ITU-T.P800"/>. Although it is typically used with a high-quality reference, the test is also capable of assessing degradation caused by codecs when tested on mild-to-moderately impaired real-world data <xref target="MULLER2024"/>. The approach is more sensitive than absolute category ratings (ACR) <xref target="ITU-T.P800"/>. An implementation of the test procedure for crowdsourced tests is available in <xref target="ITU-T.P808"/>.</t>
        </section>
        <section anchor="drt">
          <name>DRT</name>
          <t>The diagnostic rhyme test (DRT) <xref target="ITU-T.P807"/> measures speech intelligibility by presenting minimal pairs where the contrasted phonemes differ in terms of a specific, controlled phonetic category. The linguistic and acoustic insight of the DRT, with test items belonging to classes of distinctive
linguistic features which are acoustically interpretable, poses a useful tool for both codec analysis and benchmarking. The test is free from context-effects and memory effects and has a high test sensitivity. It is therefore well-suited for a crowdsourced listener audience. Bearing in mind the principles for crowdsourcing listening tests employed in <xref target="ITU-T.P808"/>, the test was adapted for crowdsourced listening tests in <xref target="CROWDSOURCED-DRT"/> and test vectors in five languages were published <xref target="DRT-REPO"/>. The test data was recently adopted by <xref target="LESCHANOWSKY2025"/>.</t>
        </section>
        <section anchor="crowdsourcing-adaptations">
          <name>Crowdsourcing Adaptations</name>
          <t>Crowdsourced listening tests benefit from rigorous screening and quality control. In addition to the specific implementation of standardized test approaches for crowdsourced listening tests, <xref target="ITU-T.P808"/> has provided useful guiding principles for the adaptation of laboratory-based tests to counteract challenges  posed by the comparatively uncontrolled crowdsourcing environment. For instance, steps of qualification and training are added before the actual test stimuli are presented and catch trials are included in the pool of test questions.
It is further recommended to assess the quality of participants' responses across different platforms, such as Amazon Mechanical Turk, Prolific, and others <xref target="CROWDSOURCED-MUSHRA"/>. Each platform has a unique set of filters that can be used to recruit a specific participant pool. The platform and any filters used should always be reported along with test results, as absolute results may depend on those settings and may differ considerably between platforms.</t>
        </section>
      </section>
    </section>
    <section anchor="proposed-crowdsourced-listening-test-battery">
      <name>Proposed Crowdsourced Listening Test Battery</name>
      <t>In the literature, evaluations of speech codec quality often focus solely on clean conditions. 
However, given the wide range of potential applications for modern speech codecs, 
and the unique ways in which ML codecs may be affected by various types of real-world distortions,
it is important to assess their limitations under representative real-world scenarios, 
including challenging listening conditions.</t>
      <t>In addition to clean speech data, the proposed test battery considers performance evaluation on overlapping speech, reverberant and noisy speech, speaker consistency and phoneme-level intelligibility. The current version comprises predominantly English test vectors, but the extension to include multiple languages is desirable.
Some of the modules of the test battery outlined below for assessment of standalone ML codec performance can also be used (where applicable), for assessing the performance of redundancy schemes under packet loss conditions (e.g., Opus+DRED).</t>
      <t>All proposed test vectors will be made publicly available at the sampling rate of 48 kHz.</t>
      <section anchor="speech-quality-evaluation">
        <name>Speech Quality Evaluation</name>
        <section anchor="clean-speech-test-vectors">
          <name>Clean Speech Test Vectors</name>
          <t>By employing the MUSHRA 1S approach and utilizing high-quality clean speech data, the system under test is evaluated with respect to the overall quality. The reference allows the listener to assess also the correctness of the linguistic content as well as the preservation of the speaker characteristics. In this test, the quality of each codec or extension is assessed in standalone mode. The diverse test set comprises 100 gender-balanced clean speech files, covering 100 unique speakers, and includes samples from both adult and children's speech. Furthermore, the set of test vectors covers a diverse range of accents of English.</t>
        </section>
        <section anchor="real-world-degradation-test-vectors">
          <name>Real-World Degradation Test Vectors</name>
          <t>As speech codecs may be used by a wide variety of applications, it cannot be ensured that the audio to be compressed constitutes clean speech in the sense of dry and noise-free high-quality audio. It is therefore important to assess the codec's resilience to real-world degradation. 
For tests where test vectors have impaired quality, DCR offers an effective way to measure the severity of any additional degradation introduced by the codec. 
The test data consists of 90 crowdsourced speech files in mildly impaired real-world scenarios of noise and reverberation. Of these, 45 files are predominantly focussed on reverberant speech and 45 on speech in noise. The reverberation and noise levels are mild to moderate.</t>
        </section>
        <section anchor="simultaneous-talker-test-vectors">
          <name>Simultaneous Talker Test Vectors</name>
          <t>Most application purposes rely on the codec's capability of preserving simultaneously occurring speech from multiple talkers. However, in practice, this can be a challenging task. A listening test using the DCR methodology offers insights into whether the presence of overlapping speech leads to degradation, which may occur in the form of artifacts or speech suppression. The proposed test set consists of 20 files of conversations between two to three talkers.</t>
        </section>
        <section anchor="packet-loss-scenarios">
          <name>Packet Loss Scenarios</name>
          <t>Real-world packet loss traces and/or simulated loss patterns (including using the packet loss simulator provided by the working group in Opus) can be utilized to evaluate the overall quality of redundancy codecs, such as Opus and DRED working together.</t>
          <t>Details TBD.</t>
        </section>
      </section>
      <section anchor="speech-intelligibility-evaluation">
        <name>Speech Intelligibility Evaluation</name>
        <section anchor="clean-speech-test-vectors-1">
          <name>Clean Speech Test Vectors</name>
          <t>The DRT for evaluating speech intelligibility, adapted for crowdsourced participants <xref target="CROWDSOURCED-DRT"/>, is proposed to be performed on a subset of the stimuli provided in <xref target="DRT-REPO"/>. The subset consists of two test vectors, one male and one female talker sample, for each word pair in the standard DRT word list for English <xref target="ITU-T.P807"/>. Test vectors for four other languages are also available in the same collection. 
Due to listeners' perceptual sensitivity to the subtle and highly localized cues that distinguish the two target phonemes, this test is primarily applicable in the evaluation of standalone codecs, with limited expected utility when combined with packet losses and redundancy schemes.</t>
        </section>
        <section anchor="noisy-test-vectors">
          <name>Noisy Test Vectors</name>
          <t>In order to evaluate a codec's resilience to noise in terms of speech intelligibility, the proposed evaluation battery for ML codecs contains noisy counterparts for the clean speech test vectors described in the previous paragraph. Speech-shaped noise (SSN) is used as a stationary additive masker in which intelligibility can be evaluated. While the presence of noise may lead to particularly severe codec distortion in some models, even the presence of well-preserved noise can help to distinguish the intelligibility of high-quality models that demonstrate a ceiling effect in clean conditions. The use of stationary noise is essential for the DRT to ensure uniform effects on the short-term localized perceptual cues. For the same reason, the noisy version of the test is also geared towards the evaluation of standalone codecs. 
The SSN was generated based on long-term-averaged short-term spectra of a publicly available clean speech data set <xref target="DEMIRSAHIN2020"/>. 
The average spectrum was used as a filter that was convolved with white noise, resulting in SSN.</t>
        </section>
      </section>
      <section anchor="example-results">
        <name>Example Results</name>
        <t>The results shown in Table 1 below were obtained by using test methodology described above. Subjective tests were run on the Prolific crowdsourcing platform. The participants were required to be native speakers of English, with an approval rate of at least 98% and at least 110 previous submissions. Only participants without any self-reported hearing impairments and without a cochlear implant were invited to participate. Additionally, diagnostic rhyme test studies were only open to participants who self-reported not to have have dyslexia.</t>
        <table>
          <thead>
            <tr>
              <th align="left">Codec</th>
              <th align="center">Quality in Clean Speech (MUSHRA) [95% CI]</th>
              <th align="center">Intelligibility in Clean Speech (DRT Score) [95% CI]</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left">Clean input</td>
              <td align="center">98.3 [+/- 0.2]</td>
              <td align="center">94.9 [+/- 1.3]</td>
            </tr>
            <tr>
              <td align="left">Opus v1.5.2 9kbps NOLACE</td>
              <td align="center">85.4 [+/- 1.7]</td>
              <td align="center">90.0 [+/- 2.0]</td>
            </tr>
            <tr>
              <td align="left">Opus v1.5.2 9kbps LACE</td>
              <td align="center">70.2 [+/- 2.0]</td>
              <td align="center">90.6 [+/- 1.8]</td>
            </tr>
            <tr>
              <td align="left">Opus v1.5.2 9kbps</td>
              <td align="center">56.2 [+/- 2.3]</td>
              <td align="center">89.0 [+/- 2.0]</td>
            </tr>
            <tr>
              <td align="left">Opus v1.5.2 6kbps</td>
              <td align="center">24.0 [+/- 0.7]</td>
              <td align="center">86.3 [+/- 2.4]</td>
            </tr>
            <tr>
              <td align="left">DRED SA 2kbps</td>
              <td align="center">64.9 [+/- 2.3]</td>
              <td align="center">88.4 [+/- 2.4]</td>
            </tr>
            <tr>
              <td align="left">DRED SA 1kbps</td>
              <td align="center">52.0 [+/- 2.4]</td>
              <td align="center">84.5 [+/- 2.8]</td>
            </tr>
            <tr>
              <td align="left">DRED SA 0.5kbps</td>
              <td align="center">20.7 [+/- 2.2]</td>
              <td align="center">71.7 [+/- 3.8]</td>
            </tr>
          </tbody>
        </table>
      </section>
    </section>
    <section anchor="objective-evaluation">
      <name>Objective Evaluation</name>
      <t>Objective metrics are often used during the development of speech codecs, 
with expert evaluations conducted towards the end of the development lifecycle. 
While effective for traditional DSP-based codecs, 
traditional well-established reference-based metrics, 
such as PESQ <xref target="ITU-T.P862"/>, often fail to accurately evaluate generative methods.
For instance, PESQ has been empirically shown to have an underestimation bias 
for generative models which may have high output quality but for which 
the output may also considerably differ from the reference <xref target="CROWDSOURCED-MUSHRA"/>.</t>
      <t>At present, the research into alternative metrics is flourishing 
with various innovative methods being proposed,<br/>
such as non-intrusive DNN-based metrics (e.g, <xref target="UTMOS"/>), 
metrics with non-matched references (e.g., <xref target="SCOREQ"/>), 
or composite score types of metrics (e.g., <xref target="UNI-VERSA"/>). 
While recent correlation investigations, e.g., <xref target="URGENT2025"/>, are promising, 
it is too early to include such metrics in this proposal, 
as it is yet to be seen which metrics can demonstrate both good accuracy and generalization 
to a variety of generative models and test vectors. 
Further insights in this area are of potential value for rapid, 
accessible and inexpensive evaluation of ML codecs. 
Hence, we propose to investigate which objective metrics are effective 
predictors of listener responses for the test battery components, 
and under which conditions.</t>
    </section>
    <section anchor="conventions-and-definitions">
      <name>Conventions and Definitions</name>
      <t>The key words "<bcp14>MUST</bcp14>", "<bcp14>MUST NOT</bcp14>", "<bcp14>REQUIRED</bcp14>", "<bcp14>SHALL</bcp14>", "<bcp14>SHALL
NOT</bcp14>", "<bcp14>SHOULD</bcp14>", "<bcp14>SHOULD NOT</bcp14>", "<bcp14>RECOMMENDED</bcp14>", "<bcp14>NOT RECOMMENDED</bcp14>",
"<bcp14>MAY</bcp14>", and "<bcp14>OPTIONAL</bcp14>" in this document are to be interpreted as
described in BCPÂ 14 <xref target="RFC2119"/> <xref target="RFC8174"/> when, and only when, they
appear in all capitals, as shown here.</t>
      <?line -18?>

</section>
    <section anchor="security-considerations">
      <name>Security Considerations</name>
      <t>TBD</t>
    </section>
    <section anchor="iana-considerations">
      <name>IANA Considerations</name>
      <t>This document has no IANA actions.</t>
    </section>
  </middle>
  <back>
    <references anchor="sec-combined-references">
      <name>References</name>
      <references anchor="sec-normative-references">
        <name>Normative References</name>
        <reference anchor="RFC2119">
          <front>
            <title>Key words for use in RFCs to Indicate Requirement Levels</title>
            <author fullname="S. Bradner" initials="S." surname="Bradner"/>
            <date month="March" year="1997"/>
            <abstract>
              <t>In many standards track documents several words are used to signify the requirements in the specification. These words are often capitalized. This document defines these words as they should be interpreted in IETF documents. This document specifies an Internet Best Current Practices for the Internet Community, and requests discussion and suggestions for improvements.</t>
            </abstract>
          </front>
          <seriesInfo name="BCP" value="14"/>
          <seriesInfo name="RFC" value="2119"/>
          <seriesInfo name="DOI" value="10.17487/RFC2119"/>
        </reference>
        <reference anchor="RFC8174">
          <front>
            <title>Ambiguity of Uppercase vs Lowercase in RFC 2119 Key Words</title>
            <author fullname="B. Leiba" initials="B." surname="Leiba"/>
            <date month="May" year="2017"/>
            <abstract>
              <t>RFC 2119 specifies common key words that may be used in protocol specifications. This document aims to reduce the ambiguity by clarifying that only UPPERCASE usage of the key words have the defined special meanings.</t>
            </abstract>
          </front>
          <seriesInfo name="BCP" value="14"/>
          <seriesInfo name="RFC" value="8174"/>
          <seriesInfo name="DOI" value="10.17487/RFC8174"/>
        </reference>
        <reference anchor="RFC6716">
          <front>
            <title>Definition of the Opus Audio Codec</title>
            <author fullname="JM. Valin" initials="JM." surname="Valin"/>
            <author fullname="K. Vos" initials="K." surname="Vos"/>
            <author fullname="T. Terriberry" initials="T." surname="Terriberry"/>
            <date month="September" year="2012"/>
            <abstract>
              <t>This document defines the Opus interactive speech and audio codec. Opus is designed to handle a wide range of interactive audio applications, including Voice over IP, videoconferencing, in-game chat, and even live, distributed music performances. It scales from low bitrate narrowband speech at 6 kbit/s to very high quality stereo music at 510 kbit/s. Opus uses both Linear Prediction (LP) and the Modified Discrete Cosine Transform (MDCT) to achieve good compression of both speech and music. [STANDARDS-TRACK]</t>
            </abstract>
          </front>
          <seriesInfo name="RFC" value="6716"/>
          <seriesInfo name="DOI" value="10.17487/RFC6716"/>
        </reference>
      </references>
      <references anchor="sec-informative-references">
        <name>Informative References</name>
        <reference anchor="ITU-T.P800">
          <front>
            <title>Methods for subjective determination of transmission quality</title>
            <author>
              <organization>ITU-T</organization>
            </author>
            <date year="1996" month="August"/>
          </front>
          <seriesInfo name="ITU-T" value="Recommendation P.800"/>
        </reference>
        <reference anchor="ITU-R.BS1534-3">
          <front>
            <title>Method for the subjective assessment of intermediate quality level of audio systems</title>
            <author>
              <organization>ITU-R</organization>
            </author>
            <date year="2015" month="October"/>
          </front>
          <seriesInfo name="ITU-R" value="Recommendation BS.1534-3"/>
        </reference>
        <reference anchor="ITU-T.P807">
          <front>
            <title>Subjective test methodology for assessing speech intelligibility</title>
            <author>
              <organization>ITU-T</organization>
            </author>
            <date year="2016" month="February"/>
          </front>
          <seriesInfo name="ITU-T" value="Recommendation P.807"/>
        </reference>
        <reference anchor="ITU-T.P808">
          <front>
            <title>Subjective evaluation of speech quality with a crowdsourcing approach</title>
            <author>
              <organization>ITU-T</organization>
            </author>
            <date year="2021" month="June"/>
          </front>
          <seriesInfo name="ITU-T" value="Recommendation P.808"/>
        </reference>
        <reference anchor="ITU-T.P862" target="https://www.itu.int/rec/T-REC-P.862">
          <front>
            <title>Perceptual evaluation of speech quality (PESQ): An objective method for end-to-end speech quality assessment of narrow-band telephone networks and speech codecs</title>
            <author>
              <organization>ITU-T</organization>
            </author>
            <date year="2001" month="February"/>
          </front>
        </reference>
        <reference anchor="CROWDSOURCED-DRT" target="https://ieeexplore.ieee.org/document/10447869">
          <front>
            <title>Crowdsourced Multilingual Speech Intelligibility Testing</title>
            <author initials="L." surname="Lechler" fullname="L. Lechler">
              <organization/>
            </author>
            <author initials="K." surname="Wojcicki" fullname="K. Wojcicki">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="ICASSP" value="2024"/>
          <seriesInfo name="DOI" value="10.1109/ICASSP48485.2024.10447869"/>
        </reference>
        <reference anchor="LESCHANOWSKY2025" target="https://arxiv.org/abs/2506.01731v1">
          <front>
            <title>Benchmarking Neural Speech Codec Intelligibility with SITool</title>
            <author initials="A." surname="Leschanowsky" fullname="A. Leschanowsky">
              <organization/>
            </author>
            <author initials="K.K." surname="Lakshminarayana" fullname="K.K. Lakshminarayana">
              <organization/>
            </author>
            <author initials="A." surname="Rajasekhar" fullname="A. Rajasekhar">
              <organization/>
            </author>
            <author initials="L." surname="Behringer" fullname="L. Behringer">
              <organization/>
            </author>
            <author initials="I." surname="Kilinc" fullname="I. Kilinc">
              <organization/>
            </author>
            <author initials="G." surname="Fuchs" fullname="G. Fuchs">
              <organization/>
            </author>
            <author initials="E.A.P." surname="Habets" fullname="E.A.P. Habets">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="INTERSPEECH" value="2025"/>
          <seriesInfo name="DOI" value="10.48550/arXiv.2506.01731"/>
        </reference>
        <reference anchor="CROWDSOURCED-MUSHRA" target="https://arxiv.org/abs/2506.00950">
          <front>
            <title>Crowdsourcing MUSHRA Tests in the Age of Generative Speech Technologies: A Comparative Analysis of Subjective and Objective Testing Methods</title>
            <author initials="L." surname="Lechler" fullname="L. Lechler">
              <organization/>
            </author>
            <author initials="C." surname="Moradi" fullname="C. Moradi">
              <organization/>
            </author>
            <author initials="I." surname="Balic" fullname="I. Balic">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="INTERSPEECH" value="2025"/>
        </reference>
        <reference anchor="COOPER2023" target="https://www.isca-archive.org/interspeech_2023/cooper23_interspeech.pdf">
          <front>
            <title>Investigating Range-Equalizing Bias in Mean Opinion Score Ratings of Synthesized Speech</title>
            <author initials="E." surname="Cooper" fullname="E. Cooper">
              <organization/>
            </author>
            <author initials="J." surname="Yamagishi" fullname="J. Yamagishi">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="INTERSPEECH" value="2023"/>
          <seriesInfo name="pages" value="1104--1108"/>
        </reference>
        <reference anchor="DRT-REPO" target="https://github.com/cisco/multilingual-speech-testing/tree/main/speech-intelligibility-DRT">
          <front>
            <title>Multilingual Speech Testing - Speech Intelligibility DRT</title>
            <author>
              <organization>Cisco Systems</organization>
            </author>
            <date>n.d.</date>
          </front>
        </reference>
        <reference anchor="MULLER2024" target="https://www.isca-archive.org/interspeech_2024/muller24c_interspeech.pdf">
          <front>
            <title>Speech quality evaluation of neural audio codecs</title>
            <author initials="T." surname="Muller" fullname="T. Muller">
              <organization/>
            </author>
            <author initials="S." surname="Ragot" fullname="S. Ragot">
              <organization/>
            </author>
            <author initials="L." surname="Gros" fullname="L. Gros">
              <organization/>
            </author>
            <author initials="P." surname="Philippe" fullname="P. Philippe">
              <organization/>
            </author>
            <author initials="P." surname="Scalart" fullname="P. Scalart">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="INTERSPEECH" value="2024"/>
          <seriesInfo name="pages" value="1760--1764"/>
        </reference>
        <reference anchor="URGENT2025">
          <front>
            <title>Interspeech 2025 URGENT Speech Enhancement Challenge</title>
            <author initials="K." surname="Saijo" fullname="K. Saijo">
              <organization/>
            </author>
            <author initials="W." surname="Zhang" fullname="W. Zhang">
              <organization/>
            </author>
            <author initials="S." surname="Cornell" fullname="S. Cornell">
              <organization/>
            </author>
            <author initials="R." surname="Scheibler" fullname="R. Scheibler">
              <organization/>
            </author>
            <author initials="C." surname="Li" fullname="C. Li">
              <organization/>
            </author>
            <author initials="Z." surname="Ni" fullname="Z. Ni">
              <organization/>
            </author>
            <author initials="A." surname="Kumar" fullname="A. Kumar">
              <organization/>
            </author>
            <author initials="M." surname="Sach" fullname="M. Sach">
              <organization/>
            </author>
            <author initials="Y." surname="Fu" fullname="Y. Fu">
              <organization/>
            </author>
            <author initials="W." surname="Wang" fullname="W. Wang">
              <organization/>
            </author>
            <author initials="T." surname="Fingscheidt" fullname="T. Fingscheidt">
              <organization/>
            </author>
            <author initials="S." surname="Watanabe" fullname="S. Watanabe">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="INTERSPEECH" value="2025"/>
          <seriesInfo name="target" value="https://arxiv.org/abs/2505.23212"/>
        </reference>
        <reference anchor="UNI-VERSA">
          <front>
            <title>Uni-VERSA: Versatile Speech Assessment with a Unified Network</title>
            <author initials="J." surname="Shi" fullname="J. Shi">
              <organization/>
            </author>
            <author initials="H.J." surname="Shim" fullname="H.J. Shim">
              <organization/>
            </author>
            <author initials="S." surname="Watanabe" fullname="S. Watanabe">
              <organization/>
            </author>
            <date year="2025"/>
          </front>
          <seriesInfo name="DOI" value="10.48550/arXiv.2505.20741"/>
          <seriesInfo name="target" value="https://arxiv.org/abs/2505.20741"/>
        </reference>
        <reference anchor="DEMIRSAHIN2020" target="https://www.aclweb.org/anthology/2020.lrec-1.804">
          <front>
            <title>Crowdsourced high-quality UK and Ireland English Dialect speech data set.</title>
            <author initials="I." surname="Demirsahin" fullname="I. Demirsahin">
              <organization/>
            </author>
            <author initials="O." surname="Kjartansson" fullname="O. Kjartansson">
              <organization/>
            </author>
            <author initials="A." surname="Gutkin" fullname="A. Gutkin">
              <organization/>
            </author>
            <author initials="C." surname="Rivera" fullname="C. Rivera">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="LREC" value="2020"/>
          <seriesInfo name="pages" value="6532--6541"/>
          <seriesInfo name="ISBN" value="979-10-95546-34-4"/>
        </reference>
        <reference anchor="SCOREQ" target="https://proceedings.neurips.cc/paper_files/paper/2024/file/bece7e02455a628b770e49fcfa791147-Paper-Conference.pdf">
          <front>
            <title>SCOREQ: Speech Quality Assessment with Contrastive Regression</title>
            <author initials="A." surname="Ragano" fullname="A. Ragano">
              <organization/>
            </author>
            <author initials="J." surname="Skoglund" fullname="J. Skoglund">
              <organization/>
            </author>
            <author initials="A." surname="Hines" fullname="A. Hines">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="NeurIPS" value="2024"/>
          <seriesInfo name="pages" value="105702--105729"/>
        </reference>
        <reference anchor="UTMOS" target="https://www.isca-archive.org/interspeech_2022/saeki22c_interspeech.pdf">
          <front>
            <title>UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022</title>
            <author initials="T." surname="Saeki" fullname="T. Saeki">
              <organization/>
            </author>
            <author initials="D." surname="Xin" fullname="D. Xin">
              <organization/>
            </author>
            <author initials="W." surname="Nakata" fullname="W. Nakata">
              <organization/>
            </author>
            <author initials="T." surname="Koriyama" fullname="T. Koriyama">
              <organization/>
            </author>
            <author initials="S." surname="Takamichi" fullname="S. Takamichi">
              <organization/>
            </author>
            <author initials="H." surname="Saruwatari" fullname="H. Saruwatari">
              <organization/>
            </author>
            <date>n.d.</date>
          </front>
          <seriesInfo name="INTERSPEECH" value="2022"/>
          <seriesInfo name="pages" value="4521--4525"/>
        </reference>
      </references>
    </references>
  </back>
  <!-- ##markdown-source:
H4sIAAAAAAAAA51c6XLbSJL+z6fAqmOi5R0SImXqjImZpQ63tdY1otyenomJ
iSJQJNECAQ4OyWy332WfZZ9sv8ysAgogqW5v/2hLAKqQleeXB9Tr9TpFVMT6
1Nt51Hnhnami0NnKm6aZd7csc+/m2jtPQx14l58LneRRmuQ7HTWZZPoZa25i
vklLzcqdTqAKPUuz1akXJdO00wnTIFELvCHM1LToxTqYxzrrLeKAlvYKrO1N
ZHGv3+/k5WQR5fSiYrXEqqvLx3ee952n4jzFG6Mk1EuN/yXFTtfb0WFUpFmk
YvrlanSGf0D5ztXD47udTlIuJjo77YSg6LQTgHScoMxPvSIrdQf0v+2oTCvs
Olou4wiE0/E8lYTeg1Zx7zFa6J3OS5o9zbK0XNJ5VTCPEu1da5UlUTJjPo3K
MEqJS7iw03nSK6wITztez1uU+TxT9FOYFfSPflZxya/pPOukBFWe9w17e57w
ZOcTaKJHfqC1dH2hohjXDVP/K9LF1E8zXqKyYI5b86JY5qd7e/QkXYqetW8f
26MLe5Msfcn1ntljb6fTUWUxTzM6CvbxvGkZxyLKnWtVZgqksix3+C72UUn0
C5/u1DuP8iD1xqu80Iuc72tDY2w04L8CesQP0sXOhhd8UIso9j6lPwdR8BR9
2xueaO1L+rP7hk6SZgusfGaeP7w73x8MTsyPx4Ojofnx8GhweNrpkOY6j189
fuw9+vfH/f4pv8iazI0Gf8KcJQW9/VkHtMILNZR5ESVMqJdOoW8qyY1We/8u
VRwVKzlSxWHPHPBUXsYXWG+9wcnJYa9/zFdynUU6J+rsEn76FPqKUy5gF/LO
ex+0GsIf/LPx4ODtsPd2E/FMezHXLv0qz3WeY7eCiI8SOg0MDdRY4r1YP+uY
7ipW0Fyk8OqZHpwz7fcHB71B/5UzPayd6WzsyzFcgRw1zzSuD0F+xVvwGdM4
nYlLk5OR5eRLDTXkw8VxNIsm0TcJBQeAUPa/WShHLvHHW4mv/QTx2NBqef8S
FXNPeQHMNczTMgvoPGq5zFJ4kG84wf6g1z/85hMcOyc43G+e4F5ngV4WoPP1
E+zeX47/+ubUG+FmdeZFrY94Ya9Ie/invbKpmonKwAREDzwIOerlPIUDTXRB
PlscuVnPLm2jfva2sac/2CbgQmUzXZx61qW+vLz4UVH6UKa9DJ7zsfdwed4D
sw5p/fnD3aeL8d3Hh/PLi97Fw2OTZeeVFHXo3ZRxAT1MZsTBsVB+1dRQj2Kt
CQbrRxHnee1bz9y68cGvPOo2uZ+PxuN71o6huXRxdwUn1PcHg/7JntwfHg+P
D3x6xh/0h8Oj48OTjYyJtNafl3GaUajRmkMN4EBJ8ttzVl5fjs/fj27vPo0/
/IRdD5osOtNJMF8oiXi3GoGnYo5AkzaL2EDGV49pGr/GphGxKQ/mKkHce1qt
8QrculZP+Zw8eaZWKlHrGzyon1Wun+aqzWrI4EzPM5C8JoUr3/tAUg5a13/w
vXdlMM9bly/9kX/ve+/VRBf5NrHdPl4+jO8vL8/fs+wOWrKDuA76iPJ/i579
/YP+od8fHL0dbJSZyj7jIQYFk3yvfvh50Nblm4/j9w+jbepMwpInWGVzuFqO
M6OZJtP9QSc64wBrZfmI/yXkqnE28BayXSyVeWSUqHiVRzmtdNwkGfhd9Zux
DM9E5f+XhZz73k2aqTBal9kZPFDwTfz/PaztnxxQGDy/u7u/fMDKVpC+Sp7p
VDPFJ3tQUKfeJTvDX+jCWaSYsTdaJcDrUUL+dhzA4PAsLRGOrRJwPo9+gYsR
Zr/Gm0sfrE+Xa6z5b9/7SS3ULMrnW71Hiw1vzfWlmpFM4UGGvR7+f7zdjeaB
6ll4Spxi9CFO/F+0417AtO2//Zdzx1+GU2wJ5wrPe3/XwjkbnKpVld42L4ut
tkbSdeDZPskMDqicEPDcYwi6t3Bo6AnNnPjgyl6RaU2oPNkzN1qYhGIG3nLz
8fqaNWTYQg3NANmMu4l4SwFpG2Kg15Lxo08xaN0sxuTpZmmxbkbIQNoOC87q
fg7Sl0u9fmccKKQfxe9UoGFLgY4O+1Cgo8Nhg+0736JBQxIGTrg/DNoqRKz5
+PDD5e3jehC6qh9lAzcPWv25TBBGAs3A5Hyu8AJY6qusRnQZq+jntHX5k+/9
HVvN1gVwnmYJ9KJ144FYOtfRZKMvu277sb/73m37GsLYh3KxFsFuiMBg3rr6
EwWpdZo/rZMMXXpHHoioC9uaM6YlBULqRH9jSPtNpwpg8nZ/QMjr4+1V70ds
0opRH5PIXPZ+hExhLHEVhEY1vjQwG09PIzjOW0GVr8oULnI8b7P3vS+XF7/B
ggqWH2zhyJZgTkDsaDj4BvaYxy8ub67AhfdXt3hp/xVYOo9m8551MB8/cNi9
ynRM/14msxgRwbuIFPL6wuJtHEbhCIX/Kr8QVS/0IoIM5lHSuncHrfwZngKZ
c562b0JlfyiLp7VFUPkH2H2mtrDwGsCcedxvOpadw4O3+73e4cFwsGP1b3x2
e+qdHJ0gT+2dHBwMD3tIPpuOx/U7Kohf9EQ4jXDLOecevcmPkRP0BkidaPH4
/O7h8q8tFy7XrA7+1XC6rYvnaVJkKmes86BnmeZ6wm8gXLht4NsNevqUzuIy
CddXvI8SvQ1nEvq+uh9v9M39g6M+eEj/7m9OB5ChBlpTKSv3KTJFy9wPgr2l
QkD/1xRWmMvPe+ym6cLeRAf6SOPXgwN1uH88OTrq6+HJNJiqo5PBYHjUu6cF
PbBmqjPkCdpAgY+PN3fjltnzJdxJn1Zpb6yy8lpNTBjnrPPHNAo0nqkdOB1z
/zUGP5KP1E9tk7/wvb+t6Sa85K16gl2s7/EhzaIVsNW6h3hUVMwKNjgVj07w
gu2y34vG9psCGx7sD3o9/H8zWP09sXR/L6fT7++vhdKO7/udTq/X8+B2oLVB
0ek8zgHgbfrnQRmWKRS8UaMhj8Kug6sADSizMOXR2JZHd2+u3wim8XRVm+52
cqRQHlAx5Rqh1ksDfjIdQtsRo1fe7sXD5cWbLhuVSUq41C177Zoq4BvQzwdY
RGEY607nOwKJWRqWAddvcRwtxek1yrjSZCEXk2pqqm+8F1O45aKvp6IFCE29
DpXTMojFC8osI+4QI6ZlUQLIp8tlmhVlEhWQL9imczwArzxZUXFeQJ3XwS5a
MEj7QF++mBN9/crbRsjFHIZ5HWSiccl0YpNoAcHAwdR1E7pRlV+wPksnZV7A
RzDpSxU86cKL0zz3vc7ldLqxgtUWk7drxSSyAA3eJIWPy+HvQxVTGYffVUsN
wYQTGjybc/WrDBByvClUC+zGepKCfuZjzKFYdWUp9ztXCEwl8zHHnkuWooX6
rGqfI0kIHBI7u1PemEgKdJf4rRGsCcV7OFwUVqd7mUOFNWSOvaHrESvBMlrq
mNRioQpAoNybqNxcADNiD26La8zY+02Hzsqb03NuuQO7GzUwgiHbwflniggj
bXiO0jKPQRvonFAoJtUwr8LJO+/TF1KuLjE7LQu6HBK78nKS63+XpGy5ZpUm
XdBbBDir03UoHZ+g0kWwKoFNQ6OhctZ1SvUtgftGlCoAbXJSKRIZp6FOtZkP
n24qfYpnIL3a4jrcImhzU67+mq5St0MKNTOADmgioFJ2KM+Qt4Fq4exQZhWG
Ges1LAgGB+5Aov8uo8ywvcOWHcBIqRYuCmqVKXZJ57S7sk6ffYWhOmzQhicg
nNwof6vWScbazAWb1mrLKgE8T9L1kjTK8YSQhV0nkBi4FaRJGFn5yp5BmsEW
WCgZNbrgleLQuCxiCHNcNzyz9VB5k+VQuIJUI4ebaRc64W/ELZFee5laRqGU
ZCkFpEt1MZdIe5kTDg8I5KRxLHasYWlZpIylOrJjWSZp0sNOPVL3ENSqvDSS
gt5fiWcH0dv0F+/MtIENZo9GNTqL4FjhXzTpBXMQ/iaOFqTOhp1CM9EBQWVl
Xtexoe64MJvhNtMqBMSme9X1tD/zu2BanW8a99zio5TQvn6FSM6pgckWG0PM
rKM519LmyN6SWhSgi27OUshU/GnGFs869opyOfIgPWMdTtKCRQXvk2bsCEWZ
HX4mSsKUo/DcrKU1MYWsYq6KLf0WrxOkJcicIOBEM8q0AkXHs96OFWYPRhcC
76pQop6EjngG0FTMF+Qe3rELV3gf/LRwYymtCHIj3J1LF7BAag/gX9OhJtqq
15vgGbqKOC0lEiyQ/xT0UGQAgJY2RUpevxlByf3BGyS1F2Bryyg1Mq9H9H3W
CFcF51Ue2T8WCcERHdmIyO+MS47ANR1qRQJhclUegU3UaQwKcehWsuwWQBTt
HhPl3B5iZ82dr0Qb8vkB7BGpWZIi/AWWwp5t4WMXxmEg197ixl/3FdWpvLYg
pA1hxsYNVmHHx3S9J00OEfE3ZOWqXda0zDjCLlKonAQH4AKdxSvrJzZ6V1ot
Lt0+1opPDe8eAu6WOe2hJpAK4b/vvvOu2axoPQ9ImOIy3frO1rcH4071kwe7
YZ8lHSp65ws41XPDs3l4l6uC0FlYaYSfI/EUc+BNMLxyTUwodBPpx5tXGppW
/F++NJu+cCs2RLCj2hwaiOyKhdwSNhVSenkbkdSBzR4Fq6dI48lyVaiWHF3T
Vnju8uv1Zyh6UTkrhiqMTUKdB1k0EV3e6gTvVQZFLWMF0TOdjiOyXh32C7sh
YwkUecimTXYZpjXdflzJmLWHeUGPsOtr+mjkXZrYRi6YUKp0vkUbq9UiFVIk
IHkkoAvdte0Pim8Amdw6B88+63CjpJH5zSnuPENMHHVJl4xInNdW+3sq0wa2
cITCtgxlQxs7zXpX5E7+Y5UnowYD08ZUGBsx5IFzz2nEbs6eQuAiU8fOcMv2
OXJWfr2zRRhNpyRJzmrcyD7XhPyyGT0E08UTuA9FAi6rDMb4OEYTHGamlSMA
nHuqhhqWrC7RUpF9E4Ea8aMioVLCKVRjVuKAS1e9omSTxkLpRznHoG5t/9Cx
nCih0RrJ09iUYQNEBVLgNC6LKvAQn6U5szs6f3hjTyN6iwBIDr7MxYaAlYge
gpuEcADGTZcnpy4Pdri5G78x2gVtiFj/mCeiTiZQWGWgACnxjUP0RBMfFtQv
yiXQY5fEA1VdLwT6NQcwemqVgzjCUUcQo1EeZEhpmXCE5kNv0CDfe6gsLyag
TdUWc5C5gglPNIQf6gX0p8iUcSN5CS2DkLJ0ITT0dN0Bm0SKMgE4jKqD9vWr
5PZGJXPGBW7mvEkz8dapJG50uXVUJ9NTbEIwxKIk+LwzS9Nwh2fOJgo/NCG8
WHGYqZcJ0mQbD2qtYbVgOBhws9O6zYI6kAZwmqfFeiLJTXKO6xz/AwB4UhUa
bcOSZ0YAqfdMuUW+IGBBhiY8p9yyeCEWG1W1sdr1mWmCfSfaQAsBf5BJxBBG
yIxycACwi2PgxfkDB3pBaMKplp57uxek5nZEhU5h1VuyJ1Jud/26omMLUnQT
3WQWi5zxKMYBSsAoUfxitQSCjHECfoEp4Tfq1447Y3dhXTaZQ6CWfGBygFVg
bR6M952sLNBlb0V7UPIAwqOYJ1coFJH+CpJVUcZuvsq0uMT15Uvd0qOjsMN3
WCRWacRqDPO3PMkaexJG4Qxw6mk0e2yux4YE3slfNvI6k1uCL880L2i0wNn+
GNsbBXh4FAWocWQ2X1mz2sXtBl1HACQmVcu3ZQXgr7FWdk/QA2iyR3xkjptQ
F5h6OMitoLUoO7tDnS1yCbNceECw6VbZpV1CxFpWigS4SRvxKTgaI0HgXyLI
gbIAwz+cyjgZUSBqBcNqqM5gCmlBzCpEC0IuLnFe2XH2n2rOnHJjfBTB7etY
hbmkCj4U4rZtyQMaOC1jvCKNWW6c1kmBTdkxiTZqk7NZXZ9mWos7NXG8Z+M4
rVvA/UKx3EsU3cSMZA/H2fjeVWHcEiyLVJbhbl4yBmGIurliwMFDgsKZVjQo
Q1KDrEOD58GxiB13UznrwF3XQDR0PF1Z6OjqqGPkL3QIA07X9L294zoGlYKG
TJpht2dNdUd+cErmGStq7hNSeyH9XJYW7H/5YscSrJFXdScmCX6VM3rQli5N
XffLl/ZEFBcA2NyaEzYjOpAA2875aweCNiDAFSaKRlB4KtcBcmt5ig5mXaSx
Eog2qUofFgpYW9rgV2ytgQt8fEjrztaEuE5gtyU41jmqAUSU9Rudh+UwcGsp
B8ONihFECjwW1begxwbhCxPILIFRYFaKglmdC3qS+BjoEtSTRxRJEsdrNPVQ
J89RlibEBd9716gW43BLNn5m6tQMl4v+2BIxW3zIVQ0xHT5IwPOTYmcmL6QH
a/hCewRUVMZOEeIW35YShJgA2w+5Bwv6kX/nrCNcCScPYPLpzM53GuTBQa+R
EWALF0R/T5B8SYWoCpRbfIF4ApxKNW0I05b3Rwv1S0rzSTRmR37Neyyzp653
D36KS+bqLxdStmd9lxQS7e7GG5n6bK7ZJ0+jmPpPAoJbEBqHzOCOnEDgHok5
JYZZvUHysFW1K++Uz7lWpOIXtSJ7otpvmrFA6vIys9tkLZzWVhHbpjKEseQT
BgIM0qqwvQ3xv/SAhLGqFjJhTCbYreIyewTipChvw/xbVQvzcUZVFYVoCZ8g
+nTbVWt3YNbRAuwGWwvIZ6QxmUVqiwp1ygeCqm7DLKKqEZc/cIIaUC/TgsI6
DSW531yQGTNoSpoTu6b+4xTkmftQcgmadcmR2AahKA5bYstUhyEvRx9N8Nlc
CAYGQXrSMxTwaGucRdMUoswt+ppMn+r+bI9SenA2zuHO6b3NuqD1Nc3o5bCO
ReM6W2GuM07R3V7nqhQld9tKjRZOwoXGGEyvp9+7jT4B92uohVDdxb/qyaoh
0RxI3bhRC2yDNzEl28ek5gYDZ7hU5A2ShoUpjdRy0LPjI25MRSJZSnGw6sZx
Z1I8XF0EqENuxOWjiOs1fmdMtV6D06BTZWzKJ/NmW6juhhFue3HKaXa83GlG
Wj1r8Jf8jEmjxUXsCjg1mg1ikJk3q3QsQmcLVsq6x4lQSShWdMxprbp1lV1p
HVCH94/cPYXujKiA3NAMi09eItyaUBcyNKgkIKxRoXpV2Dx4yT0X7myBquGx
9/T+F4b47bmUy/ojJkEkrKnOZKX3o7y8c7Yy0MwevU59q0yHFKqk4UhO5xup
2hYT2FhyM7pukz6KUpSwGtRii+yNvNypu8VQgNx4RgNPax/AIhZcwIV8bn0b
hXLgPGNpsqOcIbCdQWA/kT03sq/KsOaKkIjOeIecAVdBrU7J9ltxWKvKL3O/
w9qGW3Ckxm6tteRRTQ2CpqJybeF74RjkoN+nKiqYCawUcxOkyXmezOlKp5Jk
RAts9JWDmFKcsdC8Krkw2OT8RMEMxcUEc+TIYPv3NvujgUJGI5TwGglLUG8o
smmUquooda0yCLh+jx+NOzGZKX/E94m98oWTxDdUdJQ3A46NIzbPVxK/KJRo
EYMbt7pUd4AbsF2ZhNLaUFAIgzkukBXsIZjjIiW3gtRgta1KYSM+WpitKr+s
e5y3NSyE919PwbZEMjni95Rz5DA4Vn3GR3VUrNlk2moCnE3S7cqD63VVcaOq
AV6cP4DwKYsq8epxAoRtepfJ/M0pSaEMU5OV22V1ay5O063C5ziG6VHXyZSJ
UqwHJ/3WZICjyZJmxuGW2kwVv6X9Cb43O+rCmzs2ZGpdDA/MtgakO8GN8VIu
lSE30hpqaFusTl3p8wute3LeWGuBfHonr6NjMFdNucnmiGNKGyB/TejnUcXk
axpaf5NKdmYVGZEhk/pCZrCdqy5cFqvaCMahMYxw3kPLaDIic76uY/uvi/ZM
CLxcBRJx4CU5wEiqcVFuobtqACYq6fveqN2mKauISkrndsaMApqaDf0AJtkp
ncopm/i7jotokivkbNHRw0aNlA5qjZUzBlJhoEmaRsrJO5uNaOjIzGmaFKMR
pMUR12q73ze6xCNTybNMJ1PQt+C/eEklppErqBgqUr8XvHBNeGFstbjzUKu2
CyhoJk+aoNReZ0Fy9OSbS0ZIhDVqBFtz293GLMQWVa5ujLQ56RYlDFjeVLkZ
R3zJz2zo3hSpWwDJJgU2veQxNzINAkLVK4t0xqImxlzoAlAHVnB20QAz7Q8+
fj+oeZTyX2NEcesHpd3tJadGb2pTsanbaMpKEDHYUXyKku56VZS05YJKFFzF
apefzBpX71irGgic0YMyPST6Zar5V1E5E+AF3DIsoU/euUBbhTA7eEKs4rtk
vbzAYv5mUdgXLtvwQg9OwSnTxKmxPhdNCJE1KtNVLyegOk1gAtiFtLCq1t33
diyEKiytjon5BrowZzbDEXEamIm1oLSjLFLOJdg3l6SCuMfTs1UdulvDOBFi
tIA1EvCuUgNLdetD2Rq8WV1nPFs3dU3rjQ2IvnWkBgSgxYQzGX7Wsc9qzKGd
YpBpsJ7fcsbXUHCAUMhLIHBlm2oLeJCY5NbbtxlCI3t1Tj1x/uREnc4TmqYZ
R5OSmrId2Uxd82tApwY4aQwTGIfPg5JkdQo+fQnYKcbdy+dqqW1w3R2Pb99U
DSouNOWS+avMIhQaNUBEkjaDBIV2A8N4uSop8b1P3HRtxx55KYUUCjkyT+t0
oBkhGU1wKhaM8inJtRMP2tZa3L25Dm+CdXU+Imyu4yUHt5Yetw9Bk2Uu2DRT
n2IDdY+WVEPzx20G7tXzJm5p6LGeV3M4atQH+Vuem7qQlS45Dh4fZMCIhIMD
bTVxYGx+Dpb0SPccU3WMnKxWSrOVh6CGKYVzuiDKZYsUbpHAtgRn2rRlX+DM
8t9jsQaYQpG4vm8GVMxULvttKhcyzT0lQ5Whew5OXGVqUW3K2NeyYoYR8PON
z3m4YcAtRTNZLtuWCyaq1m4pcYpQ6Q7BjjR+tr4E6l0Im3TXlDBNpwbnM07E
u5TRO+RbXOLsCHqVcicO9sIK+8jED0yxhZsk6YRMXCCDgRftP9pQG7LMZHmt
v/Bg2i1ZmViNsJXlVqneVk0NDHNjr+wgk2A2ziZS17P5rZNbGodMpR8qYUAX
qpoJWAjZ4AQnx3+QGrK9MBj0axdU/4Ub6Modtdub5ETU0i44Jcp1PO1Vdea5
7ZNx2mImwZOwXgHp0UfNKuPuDKUafLYoeY7MLEX1JkoWRlXCRdNMm/u3eUFt
OsMkng1IlzppbMVEz9MWsZQS4ylOE/l/4SqP9edIAYb9aj7c/7WqKUFBGohr
VypFb7x/nBz8wTu/+ieebUO2tTXkMfjjZ3dZ59fTnv3P+bF32vyFaOLNomQJ
Vv4KGfpvvX/8ca/n9f19ev3J0D+RCwP/LW8s8PN54B/4+97J02SZe7d316Pz
Szx9fOAP7dNHvLzv9+XCvt/fttwsPsIr3Wdp8aHd7Xjb4l+9g8N6HZHoHZ+8
+tJDs25/aB/rC7HHh/bw+/5Q1jHCHo+8fbPmsGKHfdexPfHamoGlb78mZ8hr
hv6BvXDcXNP3Dyx1IMo+xJI4GtgLb2UVuSHnDwG4WP5ubaJb8bAy9TPYC4Zl
ZhObkDLrdFmVf1uNCLZ7Mw3mNk0oypVB0Y4SBJyna/vCN+lgBR8O5ymYoK6R
cORzRn8uxvemfVnR4N5emyttjzWaE2OZzZfo7644uPtwn7IM09xBfOFykfm8
IV7V4M8dsrTTvc12J29MjTme4NKLZZSZKQYJANYTKDO4S/3IhcF/9DcMOhtn
OZ2UW/wID0yXBVmoRSXUK6DF8ihPvpsnaJmM9bhtNNNa49JE0agCb5+874wK
23/tmkW5pu/ipLKgKIQmqqFl1GSNEXzoryVAvUR5bEMqSpL0ucFPM4ln8XHX
q2XW/Lrg4va2KV3uB/CXBPR949evbyBue6v6SkK+QXI0pGojfPkiX5/KQspR
0wVIoKgvg1dV88x9n3y6YD+vxtpKm2WcQUrlsS3gVX/Dgkummz596JryWYrI
CEZQ70xqmikQIGNhpwPEnKkYbUrmwjoVU8cwN+NgK12YeJ6TWhptMgsJCrsw
luvUNM5nTMA0u5ofbvAXLcotB68rbXtGhCqppt/uVKSEavr7c8YjOa1Rsjtx
B/zVDB0p4A+pJiY7BWqCH0ry9Q+1nNnsznsZc3up8i5hopWGNgxZ/+iFCKrd
UoeqmpEkVjRbYTsk9SSAheytniQUKTEf93CXhxs2ZsrV7XzCeZ9TtSup//7e
BU9jylwL40n6NIDKCLm3A8N8pD/3R/8i4PLP0OCPV4gc9PP4/ej6uvqhY54Y
v7/7eH1R/1SvPL+7ubm8vZDFuOo1LnV2bkY/7UiDY+fu/vHq7nZ0vVMJsPoo
jQewU/OViMxtMcbuNHLRs/P7//2fwRDa/x/mb9B9/Wp+ob9Ch18ooTdTEYS2
5Fcwd9UB3mRoR03HmIqyUaFimTIQJ0tFerDzP/9BnPnnqfenSbAcDP9sLtCB
GxctzxoXmWfrV9YWCxM3XNrwmoqbjestTjfpHf3U+N3y3bn4p7/wx5S9wfFf
/iwAYKxhthQSzhufkEB/zi74gavR7Wj9ZkOKc3a48qQKKgWl74BphLfzf9CA
YysxUwAA

-->

</rfc>
